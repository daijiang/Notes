{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "#### The hypothesis function\n",
    "\n",
    "$$h_\\theta (x) = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "#### Cost function\n",
    "\n",
    "$$J(\\theta_{0},\\theta_{1})=\\frac{1}{2m}\\sum_{i=1}^{m}\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)^{2}$$\n",
    "\n",
    "#### Gradient descent\n",
    "Update $\\theta_0$ and $\\theta_1$ simultaniousely, until convergence:\n",
    "$$\\theta_{j}:=\\theta_{j}-\\alpha\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta_{0},\\theta_{1})$$\n",
    "\n",
    "Multiple variables and solve the differentiation.\n",
    "Repeat until convergence:{\n",
    "$$\\theta_{j}:=\\theta_{j}-\\frac{\\alpha}{m}\\sum_{i=1}^{m}\\left(\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)x_{j}^{(i)}\\right)$$\n",
    "}\n",
    "\n",
    "Here, i = 1:m is the number of observations, j is the number of variables (with 1 as the first one), $\\alpha$ is the learning step.\n",
    "\n",
    "Variable (feature) **scaling** can help gradient descent converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matlab code\n",
    "# m = length(y); % number of training examples\n",
    "# num_iters = 1000;\n",
    "\n",
    "# % gradient descent to find thetas that will minimize cost\n",
    "# J_history = zeros(num_iters, 1); % to host cost of each comb of theta\n",
    "#for iter = 1:num_iters\n",
    "#    diffe = X * theta - y;\n",
    "#    % update both theta at the same time.\n",
    "#    % update both theta at the same time.\n",
    "#    theta(1) = theta(1) - (alpha/m) * sum(diffe .* X(:, 1));\n",
    "#    theta(2) = theta(2) - (alpha/m) * sum(diffe .* X(:, 2));\n",
    "#    J_history(iter) = sum((X * theta - y).^2)/(2*m);\n",
    "#end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Normal equation**: $\\theta = (X^TX)^{-1}X^Ty$.\n",
    "\n",
    "For large sample size, the normal equation will be very slow to calculate ($(X^TX)^{-1}$). But the gradient descent will work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "The outcomes are either 1 or 0. So, we need $0\\leq h(\\theta) \\leq 1$. We use logistic regression.\n",
    "$$h_{\\theta}(x)=g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 +\\cdots) = g(\\theta^{T}x)=P(y=1|x;\\theta)$$\n",
    "$$g(z)=\\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# % matlab code\n",
    "# h_theta = 1./(1 + exp(-X * theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function\n",
    "If we still use the same cost function as linear regression, the the $J(\\theta)$ vs $\\theta$ is non-convex, it will have many local minimum.\n",
    "\n",
    "Logistic regression cost function:\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}\\textrm{Cost}(h_{\\theta}(x^{(i)}),y^{(i)})$$\n",
    "\n",
    "$$\\textrm{Cost}(h_{\\theta}(x),y)=\\begin{cases}\n",
    "-\\log(h_{\\theta}(x)) & if\\;y=1\\\\\n",
    "-\\log(1-h_{\\theta}(x)) & if\\;y=0\n",
    "\\end{cases}$$ \n",
    "\n",
    "In this way, if y = 1 will predict = 0, then the cost is approaching infinity.\n",
    "\n",
    "Put both conditions together (y can only be either 1 or 0):\n",
    "\n",
    "$$\\textrm{Cost}(h_{\\theta}(x),y)=-y\\log(h_{\\theta}(x))-(1-y)\\log(1-h_{\\theta}(x))$$\n",
    "\n",
    "$$J(\\theta)=-\\frac{1}{m}\\left[\\sum_{i=1}^{m}y^{(i)}\\log(h_{\\theta}(x^{(i)}))-(1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)}))\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m = length(y);\n",
    "# J = (1/m) * sum(-y .* log(h_theta) - (1-y).*log(1-h_theta));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent\n",
    "Our **goal** is to get the $\\theta$ that minimize $J(\\theta)$: $\\min _\\theta J(\\theta)$.\n",
    "\n",
    "We need code that can compute $J(\\theta)$ and $\\partial J(\\theta)/\\partial \\theta_j$\n",
    "\n",
    "Repeat many times and simultaneously update all $\\theta_j$, $j = 0, 1, \\cdots, n$.\n",
    "$$\\theta_{j}:=\\theta_{j}-\\frac{\\alpha}{m}\\sum_{i=1}^{m}\\left(\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)x_{j}^{(i)}\\right)$$\n",
    "\n",
    "It looks identical to linear regression, but here $h_\\theta (x) = \\frac{1}{1+e^{-z}}$ instead of $\\theta^Tx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grad = zeros(size(theta));\n",
    "# for i=1:size(theta)\n",
    "#    grad(i) = (1/m)*sum((h_theta - y) .* X(:,i));\n",
    "# end  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization algorithm\n",
    "\n",
    "- Gradient descent\n",
    "- Conjugate gradient\n",
    "- BFGS\n",
    "- L-BFGS\n",
    "\n",
    "The 2-4 do not need to manually pick $\\alpha$ and often faster than gradient descent, but they are more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "E.g. weather: sunny (1), cloudy (2), rain (3), snow (4), etc.\n",
    "\n",
    "##### One-vs-all (one-vs-rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "If we have too many variables (features), the learned hypothesis may fit the training set very well, but fail to generalize to new examples (predict y on new wxamples).\n",
    "\n",
    "Addressing overfitting: \n",
    "\n",
    "1. Reduce number of features, either manually or by model selection. But this will throw away information if the feature indeed is important.\n",
    "2. Regularization: keep all features but reduce magnitude/values of parameters $\\theta_j$. This works well when we have many features and each contributes a bit to predicting y.\n",
    "\n",
    "### Regularized linear regression\n",
    "In regularized linear regression, we choose $\\theta$ to minimize: $$J(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)^{2}+\\lambda\\sum_{j=1}^{n}\\theta_j^2$$ Here, $\\lambda$ is the regularize parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Gradient descent**: repeat {\n",
    "$$\\theta_{0}:=\\theta_{0}-\\frac{\\alpha}{m}\\sum_{i=1}^{m}\\left(\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)x_{0}^{(i)}\\right)$$\n",
    "$$\\theta_{j}:=\\theta_{j}(1-\\frac{\\alpha \\lambda}{m})-\\frac{\\alpha}{m}\\sum_{i=1}^{m}\\left(\\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)x_{j}^{(i)}\\right), j=1,2,\\cdots,n$$\n",
    "}\n",
    "\n",
    "**Normal equation**: if $\\lambda>0$, $$\\theta=(X^{T}X+\\lambda\\begin{bmatrix}0\\\\\n",
    " & 1\\\\\n",
    " &  & 1\\\\\n",
    " &  &  & \\ddots\\\\\n",
    " &  &  &  & 1\n",
    "\\end{bmatrix})^{-1}X^{T}y$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression\n",
    "Gradient descent is similar as regularized linear regression, but remember that $h_\\theta (x) = \\frac{1}{1+e^{-z}}$ instead of $\\theta^Tx$. <cite data-cite=\"1263551/H4SJJAH6\"></cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# h_theta = sigmoid(X * theta);\n",
    "# J = (1/m) * sum(-y .* log(h_theta) - (1-y).*log(1-h_theta)) + (0.5*lambda/m)*sum(theta(2:size(theta)).^2);\n",
    "\n",
    "# grad(1) = (1/m)*sum((h_theta - y) .* X(:,1));\n",
    "# for i=2:size(theta)\n",
    "#    grad(i) = (1/m)*sum((h_theta - y) .* X(:,i)) + lambda * theta(i)/m;\n",
    "# end  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"cite2c-biblio\"></div>"
   ]
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "1263551/H4SJJAH6": {
     "DOI": "10.1890/14-0893.1",
     "URL": "http://www.esajournals.org/doi/abs/10.1890/14-0893.1",
     "abstract": "Fire suppression throughout the 20th century greatly altered plant communities in fire-dominated systems across North America. Our ability to assess these effects over the long term, however, is handicapped by the paucity of baseline data. Here, we used detailed baseline data from the 1950s to track changes in the over- and understory composition of pine-barrens vegetation growing on sandy, glacial lake-bed sediments in central Wisconsin. We expected fire suppression to favor succession to closed-canopy conditions, leading to decreases in shade-intolerant and fire-adapted species and consequent reductions in alpha and gamma diversity. We also expected beta diversity to decline due to increases in shade-tolerant, fire-sensitive, and exotic species. In fact, fire suppression has greatly altered the structure and composition of these pine-barrens communities over the past 54 years. Woody, wind-pollinated, and shade-tolerant species all increased in richness and abundance, as expected, with succession following fire suppression. Contrary to expectations, local and regional species richness increased by 12% and 26%, respectively, while Shannon beta diversity declined 24.1%. Increases in canopy coverage and number of native species appear to have driven this biotic homogenization. In contrast, increases in exotic species in our study did not promote biotic homogenization, reflecting their relative rarity across sites. Our findings highlight the key role fire plays in shaping the assembly of these pine-barrens communities.",
     "accessed": {
      "day": "16",
      "month": "4",
      "year": "2015"
     },
     "author": [
      {
       "family": "Li",
       "given": "Daijiang"
      },
      {
       "family": "Waller",
       "given": "Donald"
      }
     ],
     "container-title": "Ecology",
     "container-title-short": "Ecology",
     "id": "1263551/H4SJJAH6",
     "issue": "4",
     "issued": {
      "year": "2015"
     },
     "journalAbbreviation": "Ecology",
     "note": "00000",
     "page": "1030-1041",
     "page-first": "1030",
     "title": "Drivers of observed biotic homogenization in pine barrens of central Wisconsin",
     "type": "article-journal",
     "volume": "96"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
