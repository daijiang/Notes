{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal Vectors and Subspaces\n",
    "A basis is a set of independent vectors that span a space. Geometrically, it is a set of coordinate axes (think about x- and y-axis in the x-y plane). In choosing a basis, we tend to choose an orthogonal basis. \n",
    "\n",
    "**orthonormal basis** is an orthogonal basis with length 1.\n",
    "\n",
    "**Length squared** of vector $x$: $||x||^{2}=x_{1}^{2}+x_{2}^{2}+\\cdots+x_{n}^{2}=x^{T}x$. The length squared is the inner product of x with itself.\n",
    ".\n",
    "**orthogonal test**: The *inner product* $x^Ty=0$ if and only if $x$ and $y$ are orthogonal vectors. If $x^Ty > 0$, their angle is less than 90°. If $x^Ty < 0$, their angle is greater than 90°. If x and y are orthogonal, length square of x plut length square of y equal with length square of (x + y). Unfold the right hand and use the fact that length square is $x^Tx$, we get $x^Ty=0$.\n",
    "\n",
    "If nonzero vectors $v_1,..., v_k$ are mutually orthogonal (every vector is perpendicular to every other), then those vectors are linearly independent.\n",
    "\n",
    "**Prthogonal subspaces**Two subspaces V and W of the same space $\\mathbb{R}^{n}$ are orthogonal if every vector v in V is orthogonal to every vector w in W: $v^Tw = 0$ for all v and w. By this definition, the wall and the floor are not orthogonal, they interscet with a line. Two plants cannot be orthogonal in 3-d space.\n",
    "\n",
    "**Fundamental theorem of orthogonality**: The row space is orthogonal to the nullspace (in $\\mathbb{R}^{n}$, each row of A times x equal 0). The column space is orthogonal to the left nullspace (in $\\mathbb{R}^{m}$).\n",
    "\n",
    "Given a subspace V of $\\mathbb{R}^{n}$, the space of all vectors orthogonal to V is called the **orthogonal complement** of V. It is denoted by $V^⊥$ = “V perp.”\n",
    "\n",
    "*The nullspace $N(A)$ is the **orthogonal complement** of the row space $C(A^T)$ in $\\mathbb{R}^{n}$*: the row space contains all vectors that are orthogonal to the nullspace.  \n",
    "*The left nullspace $N(A^T)$ is the **orthogonal complement** of the column space $C(A)$ in $\\mathbb{R}^{m}$*: The column space contains all vectors that are orthogonal to the left nullspace.   \n",
    "$Ax = b$ to be solvable, $b$ must to be in the column space, or indirectly, $b$ to be perpendicular to the left nullspace $N(A^T)$.\n",
    "\n",
    "![subspaces](http://i.imgur.com/AKYbfWa.png)\n",
    "\n",
    "Figure 3.4 summarizes the fundamental theorem of linear algebra. It illustrates the true effect of a matrix--what is happening inside the multiplication Ax. **Every matrix transforms its row space onto its column space**. The nullspace is carried to the zero vector. Every Ax is in the column space. Nothing is carried to the left nullspace. The real action is between the row space and column space, and you see it by looking at a typical vector x. It has a “row space component” and a “nullspace component,” with $x = x_r + x_n$. When multiplied by A, this is $Ax = Ax_r + Ax_n$:\n",
    "The nullspace component goes to zero: $Ax_n = 0$.\n",
    "The row space component goes to the column space: $Ax_r = Ax$.\n",
    "Of course everything goes to the column space—the matrix cannot do anything else.\n",
    "\n",
    "Every vector b in the column space comes from **exactly one** vector $x_r$ in the row space. On those r-dimensional spaces A is invertible.\n",
    "\n",
    "$A^T$ goes in the opposite direction, from $\\mathbb{R}^{m}$ to $\\mathbb{R}^{n}$ and from $C(A)$ back to $C(A^T)$. Of course the transpose is not the inverse! $A^T$ moves the spaces correctly, but not the individual vectors. That honor belongs to $A^{−1}$ if it exists--and it only exists if $r= m= n$ (i.e, square matrix with full rank). We cannot ask $A^{−1}$ to bring back a whole nullspace out of the zero vector.\n",
    "\n",
    "When $A^{−1}$ fails to exist, the best substitute is the pseudoinverse $A^+$. This inverts A where that is possible: $A^+Ax = x$ for x in the row space. On the left nullspace, nothing can be done: $A^+y = 0$. Thus $A^+$ inverts A where it is invertible, and has the same rank r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![Imgur](http://i.imgur.com/MEhViPQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of line, b can also project to any subspace. \n",
    "\n",
    "### why projection?\n",
    "When $A^{−1}$ fails to exist, i.e. $Ax = b$ has no solution, the best substitute is the pseudoinverse $A^+$ -- solve the closest problem we can solve. $Ax$ will always be in the column space of A, but b is not necessary to be in the column space (thus no solution). So we can choose the closest vector (compare with b) in the column space. So we will solve $A\\hat{x} = P$ instead, where $P$ is the projection of $b$ onto the column space. $\\hat{x}$ is the best possible solution. \n",
    "\n",
    "In linear regression $y ~ x$, this is saying that it is impossible to link every points with one line, instead we will try to find a line with least square of errors.\n",
    "\n",
    "The **cosine** of the angle between any nonzero vectors a and b is $\\cos\\theta=\\frac{a^{T}b}{||a||||b||}$. Because $|\\cos\\theta| \\leq 1$, this gives the **Schwarz inequality**: $|a^Tb| \\leq ||a||||b||$\n",
    "\n",
    "Law of cosines: $||b-a||^{2}=||b||^{2}+||a||^{2}-2||b||||a||\\cos\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection onto a line\n",
    "p must be = $\\hat{x}a$ since they are on the same line. To get p, all we need is to computer $\\hat{x}$. We know $a\\perp(b-\\hat{x}a)$ --> $a^T(b-\\hat{x}a)=0$ --> $\\hat{x}a^Ta = a^Tb$ --> $\\hat{x}=\\frac{a^Tb}{a^Ta}$. Thus, **projection onto a line** $p = \\hat{x}a = \\frac{a^Tb}{a^Ta}a$.\n",
    "\n",
    "**Projection matrix *P:*** ($p = Pb$) is the matrix that multiples $b$ and produces $p$: $p=\\frac{a^Tb}{a^Ta}a = a\\frac{a^Tb}{a^Ta} = \\frac{aa^T}{a^Ta}b$, so $P = \\frac{aa^T}{a^Ta}$. \n",
    "\n",
    "- $aa^T$ is a square matrix, $a^Ta$ is a number. So *P* is a square matrix.\n",
    "- *P* is symmetric.\n",
    "- $P^2 = P$: $P^2b$ is the projection of $Pb$ and $Pb$ is already on the line.\n",
    "- Column space of *P* consists of the line through a.\n",
    "- the rank is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Projection and least squares\n",
    "\n",
    "System of equations $Ax = b$ either has a solution or not. If b is not in the column space C(A), the system is inconsistent and Gaussian elimination fails. This failure is almost certain when there are several equations and only one unknown. For example: $2x = b_1$, $30 = b_2$, $4x=b_3$. In this example, the row space is in 3-d, but the column space is just a line through a = (2,3,4). Only b on this line (out of the whole 3-d space!) will solve these equations. Thus, chances are that there is no solution for most b.\n",
    "\n",
    "n spite of their unsolvability, inconsistent equations arise all the time in practice. They have to be solved! One possibility is to determine x from part of the system, and ignore the rest; this is hard to justify if all m equations come from the same source. Rather than expecting no error in some equations and large errors in the others, it is much better to choose the x that minimizes an average error E in the m equations.\n",
    "\n",
    "Squared error: $E^2=(2x-b_1)^2+(30-b_2)^2+(4x-b_3)^2$. If there is an exact solution, the mimimum error will be zero. When there is no solution, the minimum error is at the lowest point of a parabola, wherer the derivative is zero: $\\frac{dE^2}{dx} = 0$, solving for x, **the least-squares solution of this model system $ax=b$ with one unknown is denoted by $\\hat{x} = \\frac{a^Tb}{a^Ta}$** -- same answer as the projection on a line above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares Problems with Several Variables\n",
    "\n",
    "Now we are ready for the serious step, to project b onto a subspace—rather than just onto a line. This problem arises from $Ax = b$ when A is an m by n matrix. Instead of one column and one unknown x, the matrix now has n columns. The number m of observations is still larger than the number n of unknowns, so it must be expected that\n",
    "$Ax = b$ will be inconsistent. *Probably, there will not exist a choice of x that perfectly fits the data b*. In other words, the vector b probably will not be a combination of the columns of A; it will be outside the column space. (think about a line (column space) in 3-d space...)\n",
    "\n",
    "Again the problem is to choose x so as to minimize the error, and again this minimization will be done in the least-squares sense. The error is $E = ||Ax − b||$ , and this is exactly **the distance from $b$ to the point $Ax$ in the column space**. Searching for the least-squares solution x, which minimizes E, is the same as locating the point $p = A\\hat{x}$ that is closer to b than any other point in the column space.\n",
    "\n",
    "We may use geometry or calculus to determine $\\hat{x}$. In n dimensions, we prefer the appeal of geometry; 1. p must be the “projection of b onto the column space.” 2. The error vector $e = b − A\\hat{x}$ must be perpendicular to that space.\n",
    "\n",
    "![Imgur](http://i.imgur.com/swH0zpq.png)\n",
    "\n",
    "Two ways to find $\\hat{x}$:\n",
    "\n",
    "1. All vectors perpendicular to the column space lie in the left nullspace. Thus the error vector must be in the null space of $A^T$: $A^T(b-A\\hat{x})=0$ --> $A^TA\\hat{x}=A^Tb$.  \n",
    "2. each column vector of A is perpendicular to the error vector: $a_1^T(b-A\\hat{x})=0$, ...this is again $A^T$: $A^T(b-A\\hat{x})=0$ --> $A^TA\\hat{x}=A^Tb$.\n",
    "3. multiple the equation $Ax = b$ by $A^T$...\n",
    "\n",
    "When $Ax = b$ is inconsistent, the least-squares solution minimuzes $||Ax-b||^2$ is:\n",
    "\n",
    "- the **Normal equations** $A^TA\\hat{x}=A^Tb$. \n",
    "- The best estimate $\\hat{x} = (A^TA)^{-1}A^Tb$.\n",
    "- the projection of b onto the column space is the nearest point $p=A\\hat{x}=A(A^TA)^{-1}A^Tb$.\n",
    "    - the **projection matrix** $P=A(A^TA)^{-1}A^T$, it projects any vector b onto the column space of A.\n",
    "    - $P^2=P$, when we project again nothing is changed.\n",
    "    - $P^T=P$\n",
    "\n",
    "Example: $A=\\begin{bmatrix}1 & 2\\\\\n",
    "1 & 3\\\\\n",
    "0 & 0\n",
    "\\end{bmatrix},b=\\begin{bmatrix}4\\\\\n",
    "5\\\\\n",
    "6\n",
    "\\end{bmatrix}$, $Ax = b$ has no slution and $A^TA\\hat{x}=A^Tb$ gives the best x. (Think A as X, b as Y and x as regression coefficience...)\n",
    "\n",
    "- suppose b is actually in the column space of A, then the projection of b is just b.\n",
    "- suppose b is perpendicular to every column of A, so $A^Tb = 0$, then b projects to the zero vector (i.e. b is in the left null space, which overlap with column space at the zero vector).\n",
    "- When A is square and invertible, the column space is the whole space. Every vector projects to itself, p equals b, and $\\hat{x} = x$: $p=A(A^{T}A)^{-1}A^{T}b=AA^{-1}(A^{T})^{-1}A^{T}b=b$. This is the only case when we can take apart $(A^TA)^{-1}$ and write it as $A^{-1}(A^T)^{-1}$. When A is rectangular, this is not possible.\n",
    "- when A has only one column, we get the same formular as the projection on line.\n",
    "\n",
    "#### The Cross-Product Matrix $A^TA$\n",
    "\n",
    "- The matrix is **square and symmetric**: $(A^TA)^T=A^TA^{TT} = A^TA$.\n",
    "- Null space of $A^TA$ = N(A)\n",
    "- It is invertible exactly if A has independent columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least-squares fitting of data\n",
    "\n",
    "Suppose we do a series of experiments, and expect the output $b$ to be a linear function of the input $t$. We look for a straight line $b = C + Dt$.\n",
    "\n",
    "How to compute C and D? If there is no experimental error, then two measurements of b will determine the line $b = C + Dt$. But if there is error, we must be prepared to “average” the experiments and find an optimal line. **That line is not to be confused with the line through a on which b was projected in the previous section!** In fact, since there are two unknowns C and D to be determined, we now project onto a two-dimensional subspace. A perfect experiment would give a perfect C and D:\n",
    "$C + Dt_1 = b_1$, ... $C + Dt_m = b_m$. \n",
    "\n",
    "This is an overdetermined system, with m equations and only two unknowns. If errors are present, it will have no solution. A has two columns, and x = (C, D).\n",
    "\n",
    "$$\\begin{bmatrix}1 & t_{1}\\\\\n",
    "1 & t_{2}\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "1 & t_{m}\n",
    "\\end{bmatrix}\\begin{bmatrix}C\\\\\n",
    "D\n",
    "\\end{bmatrix}=\\begin{bmatrix}b_{1}\\\\\n",
    "b_{2}\\\\\n",
    "\\vdots\\\\\n",
    "b_{m}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The best solution ($\\hat{C}, \\hat{D}$) is the $\\hat{x}$ that minimizes the squared error $E^2=(b_1-C-Dt_1)^2+\\ldots+(b_m-C-Dt_m)^2$. The vector p = A x is as close as possible to b. Of all straight lines $b = C + Dt$, we are choosing the one that best fits the data (Figure 3.9). On the graph, **the errors are the vertical distances $b − C − Dt$ to the straight line (not perpendicular distances!)**. It is the **vertical distances** that are squared, summed, and minimized.\n",
    "\n",
    "![Imgur](http://i.imgur.com/5cwBtDA.png)\n",
    "\n",
    "**The measurements $b_1 , \\ldots, b_m$ are given at distinct points $t_1 , \\ldots ,t_m$. Then the straight line $C + Dt$ which minimizes $E^2$ comes from least squares:   \n",
    "$A^{T}A\\begin{bmatrix}\\hat{C}\\\\\n",
    "\\hat{D}\n",
    "\\end{bmatrix}=A^{T}b$ or \n",
    "$\\begin{bmatrix}m & \\sum t_{i}\\\\\n",
    "\\sum t_{i} & \\sum t_{i}^{2}\n",
    "\\end{bmatrix}\\begin{bmatrix}\\hat{C}\\\\\n",
    "\\hat{D}\n",
    "\\end{bmatrix}=\\begin{bmatrix}\\sum b_{i}\\\\\n",
    "\\sum t_{i}b_{i}\n",
    "\\end{bmatrix}$\n",
    "**\n",
    "\n",
    "Example: Three measurements b1 , b2 , b3 are marked on Figure 3.9a:  \n",
    "b = 1 at t = − 1, b = 1 at t = 1, b = 3 at t = 2.  \n",
    "The first step is to write the equations that would hold if a line could go through all three points. Then\n",
    "every $C + Dt$ would agree exactly with b:   \n",
    "$C-D=1$  \n",
    "$C+D=1$  \n",
    "$C+2D=3$  \n",
    "The first column\n",
    "of A contains 1s, and the second column contains the times $t_i$. If those equations $Ax = b$ could be solved, there would be no errors. They can’t be solved because the points are not on a line. Therefore they are solved by least squares: $\\begin{bmatrix}3 & 2\\\\\n",
    "2 & 6\n",
    "\\end{bmatrix}\\begin{bmatrix}\\hat{C}\\\\\n",
    "\\hat{D}\n",
    "\\end{bmatrix}=\\begin{bmatrix}5\\\\\n",
    "6\n",
    "\\end{bmatrix}$. The best solution is $\\hat{C} = 97 , \\hat{D} = 47$ and the best line is $9/7 + 4/7 t$.\n",
    "\n",
    "Note the beautiful connections between the two figures. The problem is the same but the art shows it differently. In Figure 3.9b, b is not a combination of the columns (1, 1, 1) and (−1, 1, 2). In Figure 3.9, the three points are not on a line. Least squares replaces points $b$ that are not on a line by points $p$ that are! Unable to solve $Ax = b$, we solve $A\\hat{x} = p$.\n",
    "\n",
    "The line 9/7 + 4t/7 has heights 5/7 , 13/7, 17/7 at the measurement times −1, 1, 2. Those points do lie on a line. Therefore the vector p = (5/7 , 13/7, 17/7) is in the column space. *This vector is the projection*. Figure 3.9b is in three dimensions (or m dimensions if there are m points, [m rows]) and Figure 3.9a is in two dimensions (or n dimensions if there are n parameters).\n",
    "\n",
    "Subtracting p from b, the errors are e = (2/7 , −6/7 , 4/7). Those are the vertical errors in Figure 3.9a, and they are the components of the dashed vector in Figure 3.9b.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Weighted least squares\n",
    "Up to now, we accepted every observation as equally reliable. *Now suppose the observations are not trusted to the same degree.* The simplest compromise is to attach different weights $w_1^2$ and $w_2^2$, and choose the $\\hat{x}_W$ that minimizes the *weighted sum of squares*: $E^2=w_1^2(x-b_1)^2 + w_2^2(x-b_2)^2 + \\ldots$..\n",
    "\n",
    "The ordinary least-squares problem leading to $\\hat{x}_W$ comes from changing $Ax = b$ to the new system $WAx = Wb$. This changes the solution from $\\hat{x}$ to $\\hat{x}_W$. The matrix $W^TW$ turns up on both sides of the weighted normal equations: The least squares solution to $WAx = Wb$ is $\\hat{x}_W$: **Weighted normal equations** $(A^TW^TWA)\\hat{x}_W = A^TW^TWb$. The matrix $C=^W$ appears in the middle.\n",
    "\n",
    "In practice, the important question is the choice of C. The best answer comes from statisticians, and originally from Gauss. We may know that the average error is zero. That is the “expected value” of the error in b--although the error is not really expected to be zero! We may also know the *average of the square* of the error; that is the *variance*. If the errors in the $b_i$ are independent of each other, and their variances are $\\sigma_i^2$, then the right weights are $w_i = 1/\\sigma_i$. *A more accurate measurement, which means a smaller variance, gets a heavier weight.*\n",
    "\n",
    "In addition to unequal reliability, *the observations may not be independent*. If the errors are coupled, then W has off-diagonal terms. The best unbiased matrix $C = W^TW$ is the **inverse of the covariance matrix**--whose *i*, *j* entry is the expected value of (error in $b_i$) times (error in $b_j$). Then the main diagonal of $C^{−1}$ contains the variances $\\sigma_i^2$, which are the average of $(\\text{error in }b_i)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Orthogonal bases and Gram-Schmidt\n",
    "The vectors $q_1,\\ldots, q_n$ are orthonormal if\n",
    "$$q_{i}^{T}q_{j}=\\begin{cases}\n",
    "0 & i\\neq j,\\;\\textrm{given the orthogonality;}\\\\\n",
    "1 & i=j,\\;\\textrm{given the normalization.}\n",
    "\\end{cases}$$\n",
    "\n",
    "A matrix with orthonormal columns will be called ***Q***. \n",
    "\n",
    "- If Q (square or rectangular) has orthonormal columns, then $Q^TQ = I$.\n",
    "- An **orthogonal matrix** is a square matrix with orthonormal columns. Then $Q^T=Q^{-1}$, as $Q^TQ = I$.\n",
    "- When Q is rectangular, $Q^TQ = I$ but $Q^T$ is only a left-inverse.\n",
    "- Any permutation matrix P is an orthogonal matrix. \n",
    "- Multiplication by an Q preserves lengths: $||Qx|| = ||x||$ for every vector x. $(Qx)^T(Qx) = x^TQ^TQx=x^Tx$\n",
    "- The rows of a square matrix are orthonormal whenever the columns are. \n",
    "\n",
    "If we have a basis, then any vector is a combination of the basis vectors. This is exceptionally simple for an orthonormal basis, which will be a key idea behind Fourier series. Write b as a combination $b = x_1q_1 + x_2q_2 + \\cdots + x_nq_n = Qx$. To compute $x_1$, multiply both sides by $q_1^T$, then all terms disappear on the right hand as $q_1^Tq_j = 0$, we get $q_1^Tb = x_1q_1^Tq_1 = x_1$. The other $x_i$ can be calculated in the same way. Recombining the pieces gives back b:\n",
    "$$b=(q_1^Tb)q_1 + (q_2^Tb)q_2 + \\cdots + (q_n^Tb)q_n$$\n",
    "In matrix way: $Qx = b$, then $x = Q^{-1}b = Q^Tb = (q_1^Tb, \\cdots, q_n^Tb)$. **If columns of the matrix are not orthonormal, then we need fund $A^{-1}$ to solve $Ax = b$, which takes work. In the orthonormal case we only need $Q^T$!**\n",
    "\n",
    "#### Rectangular Matrices with Orthogonal Columns\n",
    "When $m>n$ for Q, then we cannot expect to solve $Qx = b$ exactly. We solve it by least squares. If Q has orthonormal columns, the least-squares problem becomes easy: we still have $Q^TQ = I$ so $Q^T$ is still the left-inverse of Q.\n",
    "\n",
    "- $Qx = b$, rectangular system with no solution for most b. \n",
    "- $Q^TQ\\hat{x} = Q^Tb$, normal equation for the best $\\hat{x}$, in which $Q^TQ = I$.\n",
    "- $\\hat{x} = Q^Tb$, i.e. $\\hat{x}_i = q_i^Tb$. \n",
    "- $p = Q\\hat{x}$, the projection of b is $(q_1^Tb)q_1 + (q_2^Tb)q_2 + \\cdots + (q_n^Tb)q_n$. Here, the projection does not reconstruct b (it does when m = n).\n",
    "- $p = QQ^Tb$, the projection matrix is $P = QQ^T$. Remenber $p = A\\hat{x}$ and $P=A(A^TA)^{-1}A^T$. **When the columns are orthonormal, the “cross-product matrix” $A^TA$ becomes $Q^TQ = I$. The hard part of least squares disappears when vectors are orthonormal.**  \n",
    "\n",
    "**Orthogonal matrices are crucial to numerical linear algebra**, because they introduce no instability. While lengths stay the same, roundoff is under control. Orthogonalizing vectors has become an essential technique. Probably it comes second only to elimination. And it leads to a factorization $A = QR$ that is nearly as famous as $A = LU$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gram-Schmidt Process\n",
    "Suppose you are given three independent vectors a, b, c. If they are orthonormal, life is easy. To project a vector v onto the first one, you compute $(a^Tv)a$. To project the same vector v onto the plane of the first two, you just add $(a^Tv)a + (b^Tv)b$. To project onto the span of a, b, c, you add three projections. All calculations require only the inner products $a^Tv$, $b^Tv$, and $c^Tv$. But to make this true, we are forced to say, “If they are orthonormal.” Now we propose to find **a way to make them orthonormal**.\n",
    "\n",
    "The **Gram-Schmidt process** starts with independent vectors $a_1,\\cdots, a_n$ and ends with orthonormal vectors $q_1,\\cdots, q_n$. At step *j* it subtracts from $a_j$ its components in the directions $q_1,\\cdots, q_{j−1}$ that are already settled:\n",
    "$$A_j = a_j − (q^T_1a_j)q_1 − \\cdots − (q^T_{j−1}a_j)q_{j−1}$$\n",
    "Then $q_j$ is the unit vector $A_j/||A_j||$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Factorization *A = QR*\n",
    "We started with a matrix A, whose columns were $a, b, c$. We ended with a matrix Q, whose columns are $q_1, q_2, q_3$. What is the relation between those matrices? The matrices A and Q are m by n when the n vectors are in m-dimensional space, and there has to be a third matrix that connects them.\n",
    "\n",
    "**New factorization A = QR**\n",
    "$$A=\\begin{bmatrix}a & b & c\\end{bmatrix}=\\begin{bmatrix}q_{1} & q_{2} & q_{3}\\end{bmatrix}\\begin{bmatrix}q_{1}^{T}a & q_{1}^{T}b & q_{1}^{T}c\\\\\n",
    "0 & q_{2}^{T}b & q_{2}^{T}c\\\\\n",
    "0 & 0 & q_{3}^{T}c\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The $QR$ factorization is like $A = LU$, except that the first factor Q has orthonormal columns. The second factor is called R, because the nonzeros are to the right of the diagonal (and the letter U is already taken). Maybe QR is not as beautiful as LU (because of the square roots). **Both factorizations are vitally important to the theory of linear algebra, and absolutely central to the calculations. If LU is Hertz, then QR is Avis.**\n",
    "\n",
    "Every m by n matrix with independent columns can be factored into $A = QR$. The columns of Q are orthonormal, and R is upper triangular and invertible. When $m = n$ and all matrices are square, Q becomes an orthogonal matrix.\n",
    "\n",
    "**Main point of orthogonalization: it simplifies the least-squares problem $Ax = b$**. The normal equations are still correct, but $A^TA$ becomes easier: $A^TA = R^TQ^TQR = R^TR$. The fundamental equation $A^TAxb = A^Tb$ simplifies to a triangular system: $R^TRxb= R^TQ^Tb$ or $Rxb= Q^Tb$. Instead of solving $Ax = QRx = b$, which can’t be done, we solve $Rxb = Q^Tb$ which is just back-substitution because R is triangular. The real cost is the $mn^2$ operations of GramSchmidt, which are needed to find Q and R in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Spaces and Fourier Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
