{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter begins the “second half” of linear algebra. The first half was about $Ax = b$. The new problem $Ax = λx$ will still be solved by simplifying a matrix—making it diagonal if possible. The basic step is no longer to subtract a multiple of one row from another: Elimination changes the eigenvalues, which we don’t want.\n",
    "\n",
    "Determinants give a transition from $Ax = b$ to $Ax = λx$. In both cases the determinant leads to a “formal solution”: to Cramer’s rule for $x = A^{−1}b$, and to the polynomial $\\det(A − λI)$, whose roots will be the eigenvalues. (**All matrices are now square**; the eigenvalues of a rectangular matrix make no more sense than its determinant.) The determinant can actually be used if n = 2 or 3. For large n, computing λ is more difficult than solving $Ax = b$.\n",
    "\n",
    "**Think A as a function** (e.g. x --> f(x)). Here, input is x and the output is Ax. Most output vectors are pointing to different directions as x. We are especially interested with the out vectors that in the same directions of the input vectors: Ax parallel to x. That is:$$Ax = \\lambda x$$\n",
    "Here, the number $\\lambda$ is an **eigenvalue** of the matrix A, and the vector x is the associated **eigenvector**. This means that only certain special numbers are eigenvalues, and only certain special vectors x\n",
    "are eigenvectors. Our goal is to find the eigenvalues and eigenvectors, λ’s and x’s, and to use them.\n",
    "\n",
    "$$(A-\\lambda I)x = 0$$\n",
    "The vector x is in the nullspace of $A − \\lambda I$. The number $\\lambda$ is chosen so that $A − \\lambda I$ has a nullspace. Of course, every matrix has a null space. Here, we want a *nonzero* eigenvector x. To be of any use, the nullspace of$A − \\lambda I$ must contain vectors other than zero. In short, $A − \\lambda I$ must be singular. **The number $\\lambda$ is an eigenvalue of A if and only if $A − \\lambda I$ is singular: $\\det (A − \\lambda I) = 0$. So there must be nonzero vectors x in its nullspace**.\n",
    "\n",
    "### The steps in solving $Ax = \\lambda x$:\n",
    "1. Compute the determinant of A − λI. With λ subtracted along the diagonal, this determinant is a polynomial of degree n. It starts with $(−\\lambda)^n$.\n",
    "2. Find the roots of this polynomial. The n roots are the eigenvalues of A.\n",
    "3. For each eigenvalue solve the equation $(A − \\lambda I)x = 0$. Since the determinant is zero, there are solutions other than $x = 0$. Those are the eigenvectors.\n",
    "\n",
    "#### Facts about eigenvalues\n",
    "\n",
    "- Sum of $\\lambda$'s = trace of A (: $a_{11} + a_{22} + \\cdots + a_{nn}$).\n",
    "- Product of $\\lambda$'s = $\\det A$.\n",
    "\n",
    "**Differential equation** $du/dt = Au$ has pure exponential solutions $u = e^{\\lambda t}x$; the eigenvalue gives the rate of growth or decay, and the eigenvector x develops at this rate. The other solutions will be mixtures of these pure solutions, and the mixture is adjusted to fit the initial conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A zero eigenvalue signals that A is singular (not invertible); its determinant is zero. Invertible matrices have all $λ \\neq 0$.\n",
    "- Projection matrix has eigenvalues 1 (when project to itself) and 0 (when project to the nullspace).\n",
    "- The eigenvalues are on the main diagonal when A is triangular. This points to one main theme of the chapter: **To transform A into a diagonal or triangular matrix without changing its eigenvalues**. We emphasize once more that the Gaussian factorization A = LU is not suited to this purpose. The eigenvalues of U may be visible on the diagonal, but they are not the eigenvalues of A.\n",
    "- For most matrices, there is no doubt that the eigenvalue problem is computationally more difficult than $Ax = b$.  For a 5 by 5 matrix, det(A − λI) involves $λ^5$. Galois and Abel proved that there can be no algebraic formula for the roots of a fifth-degree polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonalization of a Matrix\n",
    "Suppose the n by n matrix $A$ has n linearly independent eigenvectors. If these eigenvectors are the columns of a matrix $S$, then $S^{−1}AS$ is a diagonal matrix $\\Lambda$. The eigenvalues of A are on the diagonal of $\\Lambda$: $$S^{-1}AS= \\Lambda = \\begin{bmatrix}\\lambda_{1}\\\\\n",
    " & \\lambda_{2}\\\\\n",
    " &  & \\ddots\\\\\n",
    " &  &  & \\lambda_{n}\n",
    "\\end{bmatrix}$$\n",
    "$$A = S\\Lambda S^{-1}$$\n",
    "We call $S$ the “eigenvector matrix” and $\\Lambda$ the “eigenvalue matrix\".\n",
    "\n",
    "- **Diagonalizability of A depends on enough eigenvectors.**\n",
    "- **Invertibility of A depends on nonzero eigenvalues.**\n",
    "- **Diagonalization can fail only if there are repeated eigenvalues**. Even then, it does not always fail. A = I has repeated eigenvalues 1, 1,..., 1 but it is already diagonal.\n",
    "- If eigenvectors x1,..., xk correspond to different eigenvalues λ1,..., λk, then those eigenvectors are linearly independent.\n",
    "- If the matrix A has no repeated eigenvalues—the numbers λ1,..., λn are distinct—then its n eigenvectors are automatically independent. Therefore any matrix with distinct eigenvalues can be diagonalized.\n",
    "- The diagonalizing matrix S is not unique. An eigenvector x can be multiplied by a constant, and remains an eigenvector.\n",
    "- Not all matrices possess n linearly independent eigenvectors, so not all matrices are diagonalizable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power and product: $A^k$ and $AB$\n",
    "The eigenvalues of $A^k$ are $\\lambda_1^k,\\cdots, \\lambda_n^k$, and each eigenvector of $A$ is still an eigenvector of $A^k$. When $S$ diagonalizes $A$, it also diagonalizes $A^k$:\n",
    "$$\\Lambda^k = (S^{−1}AS)(S^{−1}AS)\\cdots (S^{−1}AS) = S^{−1}A^kS.$$\n",
    "$$A^k = S\\Lambda^kS^{-1}$$\n",
    "\n",
    "- Eigenvalues can tell us about $A^k$. e.g. $A^k \\rightarrow 0$ as $k \\rightarrow \\infty$, if all absolute values of eigenvalues < 1. \n",
    "- Diagonalizable matrices share the same eigenvector matrix S if and only if AB = BA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference equations\n",
    "Suppose you deposit 1000 dollors and the bank gives you 6% interestes. And suppose the bank is really nice and allow instant give back of your interests. Then $$p_{k+1} = (1 + 0.06 \\Delta t)p_k.$$\n",
    "$$\\frac{p_{k+1} - p_{k}}{\\Delta t} = 0.06p_k$$\n",
    "$$\\frac{dp}{dt} = 0.06p$$\n",
    "$$p(t)=e^{0.06t}p_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fibonacci numbers\n",
    "0, 1, 1, 2, 3, 5, 8, ... $x_n = x_{n-1} + x_{n-2}, for\\, n \\geq 3$. Put it in matrix form (with the trick $ x_{n-1} =  x_{n-1}$):\n",
    "$$\\begin{bmatrix}x_{n}\\\\\n",
    "x_{n-1}\n",
    "\\end{bmatrix}=\\mu_{n}=A\\mu_{n-1}=\\begin{bmatrix}1 & 1\\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\\begin{bmatrix}x_{n-1}\\\\\n",
    "x_{n-2}\n",
    "\\end{bmatrix}$$\n",
    "$$\\begin{bmatrix}x_{n}\\\\\n",
    "x_{n-1}\n",
    "\\end{bmatrix}=\\begin{bmatrix}1 & 1\\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}^{2}\\begin{bmatrix}x_{n-2}\\\\\n",
    "x_{n-3}\n",
    "\\end{bmatrix}=\\begin{bmatrix}1 & 1\\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}^{n-2}\\begin{bmatrix}1\\\\\n",
    "1\n",
    "\\end{bmatrix}=\\begin{bmatrix}1 & 1\\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}^{n-1}\\begin{bmatrix}1\\\\\n",
    "0\n",
    "\\end{bmatrix}$$\n",
    "The issue now is to calculate $A^n$ to get $x_n$. We already know that $A^n = S\\Lambda^n S^{-1}$.\n",
    "\n",
    "$$\\Lambda=\\begin{bmatrix}\\frac{1+\\sqrt{5}}{2} & 0\\\\\n",
    "0 & \\frac{1-\\sqrt{5}}{2}\n",
    "\\end{bmatrix},S=\\begin{bmatrix}\\frac{1+\\sqrt{5}}{2} & \\frac{1-\\sqrt{5}}{2}\\\\\n",
    "1 & 1\n",
    "\\end{bmatrix},S^{-1}=\\begin{bmatrix}\\frac{\\sqrt{5}}{5} & \\frac{5-\\sqrt{5}}{10}\\\\\n",
    "\\frac{-\\sqrt{5}}{5} & \\frac{5+\\sqrt{5}}{10}\n",
    "\\end{bmatrix}$$\n",
    "Then we get:\n",
    "$$x_{n}=\\frac{\\sqrt{5}}{5}\\left(\\frac{1+\\sqrt{5}}{2}\\right)^{n}-\\frac{\\sqrt{5}}{5}\\left(\\frac{1-\\sqrt{5}}{2}\\right)^{n}\\approx\\frac{\\sqrt{5}}{5}\\left(\\frac{1+\\sqrt{5}}{2}\\right)^{n}$$\n",
    "We then round to the nearest integer and that is $x_n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov matrix\n",
    "A Markov matrix A has all $a_{ij} \\geq 0$, with each column adding to 1.\n",
    "\n",
    "(a) $\\lambda_1$ = 1 is an eigenvalue of A.  \n",
    "(b) Its eigenvector $x_1$ is nonnegative — and it is a steady state, since $Ax_1 = x_1$.  \n",
    "(c) The other eigenvalues satisfy $||\\lambda_i|| \\leq 1$.  \n",
    "(d) If A or any power of A has all positive entries, these other |$\\lambda_i$| are below 1. The solution $A^ku_0$ approaches a multiple of $x_1$ —which is the steady state $\\mu_\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability of $u_{k+1} = Au_k$\n",
    "The difference equation $u_{k+1} = Au_k$ is:\n",
    "\n",
    "- stable if all eigenvalues satisfy |$\\lambda_i$| < 1;\n",
    "- neutrally stable if some |$\\lambda_i$| = 1 and all the other |$\\lambda_i$| < 1; and\n",
    "- unstable if at least one eigenvalue has |$\\lambda_i$| > 1.\n",
    "\n",
    "In the stable case, the powers $A_k$ approach zero and so does $u_k = A^ku_0$.\n",
    "\n",
    "For **differential equations** $du/dt = Au$, the solution $u(t) = e^{At}u(0)$ depends on the exponential of A.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Matrix exponential\n",
    "\n",
    "$$e^{x}=\\sum_{o}^{\\infty}\\frac{x^{n}}{n!}=1+x+\\frac{x^{2}}{2!}+\\frac{x^{3}}{3!}+\\cdots$$\n",
    "\n",
    "$$e^{At}=I+At+\\frac{(At)^{2}}{2!}+\\frac{(At)^{3}}{3!}+\\cdots=Se^{\\Lambda t}S^{-1}$$\n",
    "\n",
    "$$\\frac{1}{1-x} = \\sum_{o}^{\\infty}x^n$$\n",
    "\n",
    "$$(I-At)^{-1}=I+At+(At)^{2}+(At)^{3}+\\cdots$$\n",
    "\n",
    "$e^{At}$ is always invertible, as $(e^{At})(e^{-At}) = I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Complex matrices\n",
    "The complex conjugate of a + ib is the number a − ib. The sign of the imaginary part is reversed. $i^2=-1$. \n",
    "![Imgur](http://i.imgur.com/Q2X5IP0.png)\n",
    "\n",
    "**A Hermitian**: $A^H = \\overline{A}^T$ has entries $(A^H)_{ij} = \\overline{A_{ji}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hermitian Matrices\n",
    "We spoke in earlier chapters about symmetric matrices: $A = A^T$. With complex entries, this idea of symmetry has to be extended. The right generalization is not to matrices that equal their transpose, but to matrices that equal their conjugate transpose. These are the **Hermitian matrices**, and a typical example is A:\n",
    "$$\\textrm{Hermitian matrix}\\;A=\\begin{bmatrix}2 & 3-3i\\\\\n",
    "3+3i & 5\n",
    "\\end{bmatrix}=A^{H}$$\n",
    "*The diagonal entries must be real*; they are unchanged by conjugation. Each off-diagonal entry is matched with its mirror image across the main diagonal, and $3 − 3i$ is the conjugate of $3 + 3i$. In every case, $a_{ij} = a_{ji}$.\n",
    "\n",
    "A real symmetric matrix is certainly Hermitian. (For real matrices there is no difference between $A^T$ and $A^H$.)\n",
    "\n",
    "**Properties**:\n",
    "\n",
    "1. If $A = A^H$, then for all complex vectors x, the number xHAx is real.\n",
    "2. If $A = A^H$, every eigenvalue is real. The eigenvector x might be complex. It is when $A = A^T$ that we can be sure λ and x stay real. More than that, the eigenvectors are perpendicular: $x^Ty = 0$ in the real symmetric case and $x^Hy = 0$ in the complex Hermitian case.\n",
    "3.  Two eigenvectors of a real symmetric matrix or a Hermitian matrix, if they come from different eigenvalues, are orthogonal to one another.\n",
    "\n",
    "- The diagonalizing matrix can be chosen with orthonormal columns when $A = A^H$.\n",
    "- A real symmetric matrix can be factored into $A = Q\\Lambda Q^T$. Its orthonormal eigenvectors are in the orthogonal matrix Q and its eigenvalues are in $\\Lambda$.\n",
    "\n",
    "#### Unitary matrices\n",
    "A complex matrix with orthonormal columns is called a unitary matrix $U$:\n",
    "\n",
    "- $U^HU=I$.\n",
    "- $UU^H=I$.\n",
    "- $U^H=U^{-1}$.\n",
    "\n",
    "![Imgur](http://i.imgur.com/o7yuaaw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar matrices\n",
    "Virtually every step in this chapter has involved the combination $\\Lambda = S^{-1}AS$. When A was symmetric, we wrote Q instead of S, choosing the eigenvectors to be orthonormal. In the complex case, when A is Hermitian we write U—it is still the matrix of eigenvectors. Now we look at all combinations $M^{−1}AM$—*formed with any invertible M on the right and its inverse on the left.* The invertible eigenvector matrix S may fail to exist (the defective case), or we may not know it, or we may not want to use it.\n",
    "\n",
    "**The matrices A and $M^{−1}AM$ are “similar”**. Going from one to the other is a **similarity transformation**. It is the natural step for differential equations or matrix powers or eigenvalues—just as elimination steps were natural for $Ax = b$.\n",
    "\n",
    "**Similar matrices share the same eigenvalues**: Suppose that $B = M^{−1}AM$. Then A and B have the same eigenvalues. Every eigenvector $x$ of A corresponds to an eigenvector $M^{−1}x$ of B.\n",
    "\n",
    "#### Change of Basis = Similarity Transformation\n",
    "Similar matrices represent the same transformation T with respect so different bases.\n",
    "\n",
    "The matrices A and B that represent the same linear transformation T with respect to two different bases (the v’s and the V’s) are similar:\n",
    "$$[T]_{V\\,to\\,V} = [I]_{v\\,to\\,V}\\;[T]_{v\\,to\\,v}\\;{I}_{V\\,to\\,v}$$\n",
    "$$B = M^{-1}AM$$\n",
    "\n",
    "![Imgur](http://i.imgur.com/d52ynbX.png)\n",
    "**The way to simplify that matrix A -- in fact to diagonalize it -- is to find its eigenvectors**. They go into the columns of M (or S) and $M^{−1}AM$ is diagonal. The algebraist says the same thing in the language of linear transformations: *Choose a basis consisting of eigenvectors*. The standard basis led to A, which was not simple. The right basis led to B, which was diagonal.\n",
    "\n",
    "**Eigenvalues are actually calculated by a sequence of simple similarities**. The matrix goes gradually toward a triangular form, and the eigenvalues gradually appear on the main diagonal. This is much better than trying to compute det(A− λI), whose roots should be the eigenvalues. For a large matrix, it is numerically impossible to concentrate all that information into the polynomial and then get it out again.\n",
    "\n",
    "Fot any matrix A, there is a unitary matrix $M = U$ such that $U^{−1}AU = T$ is triangular. The eigenvalues of A appear along the diagonal of this similar matrix T.\n",
    "\n",
    "**The diagonal matrix $U^{−1}AU$ represents a key theorem in linear algebra.**\n",
    "\n",
    "(**Spectral Theorem**) Every real symmetric A can be diagonalized by an orthogonal matrix Q. Every Hermitian matrix can be diagonalized by a unitary U:\n",
    "$$\\text{(real) }Q^{−1}AQ = \\Lambda\\, or\\, A = Q\\Lambda Q^T$$\n",
    "$$\\text{(complex) }U^{−1}AU = \\Lambda\\, or\\, A = U\\Lambda U^H$$\n",
    "The columns of Q (or U) contain orthonormal eigenvectors of A.\n",
    "\n",
    "The matrix N is **normal** if it commutes with $N^H$: $NN^H = N^HN$. For such matrices, and no others, the triangular $T = U^{−1}NU$ is the diagonal $\\Lambda$. Normal matrices are exactly those that have a complete set of orthonormal eigenvectors.\n",
    "\n",
    "Symmetric and Hermitian matrices are certainly normal: If $A = A^H$, then $AA^H$ and $A^HA$ both equal $A^2$. Orthogonal and unitary matrices are also normal: $UU^H$ and $U^HU$ both equal $I$.\n",
    "\n",
    "If N is normal, then so is the triangular $T = U^{−1}NU$; A triangular T that is normal must be diagonal! Thus, if N is normal, the triangular $T = U^{−1}NU$ must be diagonal. Since T has the same eigenvalues as N, it must be Λ. The eigenvectors of N are the columns of U, and they are orthonormal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jordan form\n",
    "![Imgur](http://i.imgur.com/loTI6Ec.png)\n",
    "![Imgur](http://i.imgur.com/rMflo7y.png)\n",
    "####Properties of eigenvalues and eigenvectors\n",
    "![Imgur](http://i.imgur.com/zFHj79X.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
