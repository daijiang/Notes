{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading notes of **Ecological Models and Data in R**\n",
    "April 4, 2014\n",
    "\n",
    "Chapter 1: Introduction\n",
    "============\n",
    "\n",
    "Frameworks for statistical inference\n",
    "------------------------------------\n",
    "\n",
    "### Classical frequentist\n",
    "\n",
    "Classical statistics, which are part of the broader frequentist\n",
    "paradigm, are the kind of statistics typically presented in introductory\n",
    "statistics classes. For a specific experimental procedure (such as\n",
    "drawing cards or flipping coins), you calculate the probability of a\n",
    "particular outcome, which is defined as *the long-run average frequency\n",
    "of that outcome in a sequence of repeated experiments*. Next you\n",
    "calculate a p-value, defined as the probability of that outcome or any\n",
    "more extreme outcome given a specified null hypothesis. If this\n",
    "so-called tail probability is small, then you reject the null\n",
    "hypothesis: otherwise, you fail to reject it. But you dont accept the\n",
    "alternative hypothesis if the tail probability is large, you just fail\n",
    "to reject the null hypothesis.\n",
    "\n",
    "A frequentist would translate this biological question into statistics\n",
    "as what is the probability that I would observe a result this extreme,\n",
    "or more extreme, given the sampling procedure?\n",
    "\n",
    "Working statisticians will tell you that it is better to focus on\n",
    "estimating the values of biologically meaningful parameters and finding\n",
    "their confidence limits rather than worrying too much about whether p is\n",
    "greater or less than 0.05 (Yoccoz, 1991; Johnson, 1999; Osenberg et al.,\n",
    "2002) although Stephens et al. (2005) remind us that hypothesis testing\n",
    "can still be useful.\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "Most modern statistics uses an approach called maximum likelihood\n",
    "estimation, or approximations to it. For a particular statistical model,\n",
    "maximum likelihood finds the set of parameters (e.g. seed removal rates)\n",
    "that makes the observed data (e.g. the particular outcomes of predation\n",
    "trials) most likely to have occurred. Based on a model for both the\n",
    "deterministic and stochastic aspects of the data, we can compute the\n",
    "likelihood (the probability of the observed outcome) given a particular\n",
    "choice of parameters. We then find the set of parameters that makes the\n",
    "likelihood as large as possible, and take the resulting maximum\n",
    "likelihood estimates (MLEs) as our best guess at the parameters. For\n",
    "mathematical convenience, we often work with the logarithm of the\n",
    "likelihood (the log-likelihood ) instead of the likelihood; the\n",
    "parameters that give the maximum log-likelihood also give the maximum\n",
    "likelihood.\n",
    "\n",
    "However, most modelers add a frequentist interpretation to likelihoods,\n",
    "using a mathematical proof that says that, across the hypothetical\n",
    "repeated trials of the frequentist approach, the distribution of the\n",
    "negative logarithm of the likelihood itself follows a $\\chi^{2}$\n",
    "(chi-squared) distribution.\n",
    "\n",
    "Likelihood and classical frequentist analysis share the same\n",
    "philosophical underpinnings. Likelihood analysis is really a particular\n",
    "flavor of frequentist analysis, one that focuses on writing down a\n",
    "likelihood model and then testing for significant differences in the\n",
    "likelihood ratio rather than applying frequentist statistics directly to\n",
    "the observed outcomes. Classical analyses are usually easier because\n",
    "they are built into common statistics packages, and they may make fewer\n",
    "assumptions than likelihood analyses (for example, Fishers test is exact\n",
    "while the LRT is only valid for large data sets), but likelihood\n",
    "analyses are often better matched with ecological questions.\n",
    "\n",
    "### Bayesian\n",
    "\n",
    "Frequentist statistics assumes that there is a true state of the world\n",
    "(e.g. the difference between species in predation probability) which\n",
    "gives rise to a distribution of possible experimental outcomes. The\n",
    "Bayesian framework says instead that the experimental outcome what we\n",
    "actually saw happen is the truth, while the parameter values or\n",
    "hypotheses have probability distributions. The Bayesian framework solves\n",
    "many of the conceptual problems of frequentist statistics: answers\n",
    "depend on what we actually saw and not on a range of hypothetical\n",
    "outcomes, and we can legitimately make statements about the probability\n",
    "of different hypotheses or parameter values.\n",
    "\n",
    "Frequentists, who believe that the true value is a fixed number and\n",
    "uncertainty lies in what you observe <span>[</span>or might have\n",
    "observed<span>]</span>, and Bayesians, who believe that observations are\n",
    "fixed numbers and the true values are uncertain.\n",
    "\n",
    "Chapter 2: Explorayory data analysis and graphics\n",
    "======================================\n",
    "\n",
    "Chapter 3: Deterministic functions for ecological modeling\n",
    "===============================================\n",
    "\n",
    "Introduction\n",
    "------------\n",
    "\n",
    "What functions could fit this pattern? What do their parameters mean in\n",
    "terms of the shapes of the curves? In terms of ecology? How do we\n",
    "eyeball the data to obtain approximate parameter values, which we will\n",
    "need as a starting point for more precise estimation and as a check on\n",
    "our results?\n",
    "\n",
    "The Ricker function, $y=axe^{-bx}$, is a standard choice for\n",
    "hump-shaped ecological patterns that are skewed to the right.\n",
    "\n",
    "Finding out about functions numerically\n",
    "---------------------------------------\n",
    "\n",
    "Calculating and plotting curves in R.  \n",
    "Useful non-linear functions\n",
    "\n",
    "| Name |  | Equation |\n",
    "|---------------------------------------------|------------------------------------|-----------------------------------------|\n",
    "| Asymptotic functions |  |  |\n",
    "|  | Michaelis-Menten | $y=\\frac{ax}{1+bx}$ |\n",
    "|  | 2-parameter asymptotic exponential | $y=a(1-e^{-bx})$ |\n",
    "|  | 3-parameter asymptotic exponential | $y=a-be^{-cx}$ |\n",
    "| S-shaped functions |  |  |\n",
    "|  | logistic 2-parameter | $y=\\frac{e^{a+bx}}{1+e^{a+bx}}$ |\n",
    "|  | logistic 3-parameter | $y=\\frac{a}{1+be^{-cx}}$ |\n",
    "|  | logistic 4-parameter | $y=a+\\frac{b-a}{1+e^{(c-x)/d}}$ |\n",
    "|  | Weibull | $y=a-be^{-(cx^{d})}$ |\n",
    "|  | Gompertz | $y=ae^{-be^{-cx}}$ |\n",
    "| Humped curves |  |  |\n",
    "|  | Ricker curve | $y=axe^{-bx}$ |\n",
    "|  | First-order compartment | $y=ke^{(-e_{a}x-e^{-e_{b}x})}$ |\n",
    "|  | Bell-shaped | $y=ae^{-\\mid bx\\mid^{2}}$ |\n",
    "|  | Biexponential | $y=ae^{bx}-ce^{-dx}$ |\n",
    "| Other functions |  |  |\n",
    "|  | Negative exponential | $y=ae^{-bx}$ |\n",
    "|  | Polynomial functions | $y=\\sum_{i=0}^{n}a_{i}x^{i}$ |\n",
    "| Rational functions |  | $y=(\\sum a_{i}x^{i}) / (\\sum b_{j}x^{j})$ |\n",
    "|  | Hyperbolic | $y=a/x$ |\n",
    "|  | Michaelis-Menten (Holling type II) | $y=\\frac{ax}{b+x}$ |\n",
    "|  | Holling type III | $y=\\frac{ax^{2}}{b^{2}+x^{2}}$ |\n",
    "|  | Holling type IV | $y=\\frac{ax^{2}}{b+cx+x^{2}}$ |\n",
    "\n",
    "Finding out about functions analytically\n",
    "----------------------------------------\n",
    "\n",
    "### Taking limits: what happens at either end?\n",
    "\n",
    "Terms with larger powers of $x$ will dwarf smaller powers, and\n",
    "exponentials will dwarf any power. Exponentials are stronger than\n",
    "powers: $x^{-n}e^{x}$ eventually gets big and $x^{n}e^{-x}$ eventually\n",
    "gets small as $x$ increases, no matter how big $n$ is (exponentials\n",
    "always win). For more difficult functions that contain a fraction whose\n",
    "numerator and denominator both approach zero or infinity in some limit\n",
    "(and thus make it hard to find the limiting value), you can try\n",
    "LHopitals Rule: $\\lim\\frac{a(x)}{b(x)}=\\lim\\frac{a'(x)}{b'(x)}$.\n",
    "\n",
    "### Taylor series approximation\n",
    "\n",
    "The Taylor series or Taylor approximation is the single most useful, and\n",
    "used, application of calculus for an ecologist. Two particularly useful\n",
    "applications of Taylor approximation are understanding the shapes of\n",
    "goodnessof-fit surfaces (Chapter 6) and the delta method for estimating\n",
    "errors in estimation (Chapter 7). The Taylor series allows us to\n",
    "approximate a complicated function near a point we care about, using a\n",
    "simple function a polynomial with a few terms, say a line or a quadratic\n",
    "curve. In practice ecologists never go beyond a quadratic expansion.\n",
    "$$f(x)\\thickapprox f(x_{0})+f'(x_{0})\\cdot(x-x_{0})+f''(x_{0})\\cdot\\frac{(x-x_{0})^{2}}{2}+\\cdots+f^{n}(x_{0})\\cdot\\frac{(x-x_{0})^{n}}{n!}$$\n",
    "\n",
    "The Taylor expansion of the exponential, $e^{rx}$ , around $x=0$ is\n",
    "$1+rx+(rx)^{2}/2+(rx)^{3}/(2\\cdot3)...$. Remembering this fact rather\n",
    "than working it out every time may save you time in the long run for\n",
    "example, to understand how the Ricker function works for small $x$ we\n",
    "can substitute $(1-bx)$ for $e^{-bx}$ (dropping all but the first two\n",
    "terms!) to get $y\\thickapprox ax-abx^{2}$ : this tells us immediately\n",
    "that the function starts out linear, but starts to curve downward right\n",
    "away.\n",
    "\n",
    "Calculating Taylor approximations is often tedious (all those\n",
    "derivatives), but we usually try to do it at some special point where a\n",
    "lot of the complexity goes away (such as $x=0$ for a logistic curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 4: Probability and Stochastic Distributions for Ecological Modeling\n",
    "================================================================\n",
    "\n",
    "Probability density function: A probability distribution over a\n",
    "continuous range (such as all real numbers, or the non-negative real\n",
    "numbers) is called a *continuous* distribution. The ***cumulative\n",
    "distribution function*** of a continuous distribution\n",
    "($F(x)=Prob(X\\leq x)$ is easy to define and understand its just the\n",
    "probability that the continuous random variable $X$ is smaller than a\n",
    "particular value $x$ in any given observation or experiment but the\n",
    "***probability density function*** (the analogue of the distribution\n",
    "function for a discrete distribution) is more confusing, since the\n",
    "probability of any precise value is zero. You may imagine that a\n",
    "measurement of (say) pH is exactly 7.9, but in fact what you have\n",
    "observed is that the pH is between 7.82 and 7.98 if your meter has a\n",
    "precision of 1%. Thus continuous probability distributions are expressed\n",
    "as probability densities rather than probabilities the probability that\n",
    "random variable X is between x and x + $\\Delta$x, divided by $\\Delta$x\n",
    "(Prob(7.82 \\< X \\< 7.98)/0.16, in this case). Dividing by $\\Delta$x\n",
    "allows the observed probability density to have a well-defined limit as\n",
    "precision increases and $\\Delta$x shrinks to zero. Unlike probabilities,\n",
    "Probability densities can be larger than 1 (Figure 4.5). For example, if\n",
    "the pH probability distribution is uniform on the interval\n",
    "<span>[</span>7,7.1<span>]</span> but zero everywhere else, its\n",
    "probability density is 10. In practice, we will mostly be concerned with\n",
    "relative probabilities or likelihoods, and so the maximum density values\n",
    "and whether they are greater than or less than 1 wont matter much.\n",
    "\n",
    "In probability theory, a ***probability density function (pdf)***, or\n",
    "density of a *continuous random variable*, is a function that describes\n",
    "the *relative likelihood* for this random variable to take on a given\n",
    "value. The probability of the random variable falling within a\n",
    "particular range of values is given by the integral of this variables\n",
    "density over that rangethat is, it is given by the area under the\n",
    "density function but above the horizontal axis and between the lowest\n",
    "and greatest values of the range. The probability density function is\n",
    "nonnegative everywhere, and its integral over the entire space is equal\n",
    "to one.\n",
    "\n",
    "A ***probability density function*** is most commonly associated with\n",
    "absolutely continuous univariate distributions. A random variable $X$\n",
    "has ***density*** $f_{X}$, where $f_{X}$ is a non-negative\n",
    "*Lebesgue-integrable* function, if:\n",
    "$$Pr[a\\leq X\\leq b]=\\int_{a}^{b}f_{X}(x)dx$$ Hence, if $F_{X}$ is the\n",
    "***cumulative distribution function*** of $X$, then:\n",
    "$$F_{X}(x)=\\int_{-\\infty}^{x}f_{X}(\\mu)d\\mu$$ and (if $f_{X}$ is\n",
    "continuous at $x$) $$f_{X}(x)=\\frac{d}{dx}F_{X}(x)$$ Intuitively, one\n",
    "can think of $f_{X}(x)dx$ as being the probability of $X$ falling within\n",
    "the infinitesimal interval $[x,\\:x+dx]$.\n",
    "\n",
    "**Definition.** Let $X$ be a discrete random variable. Then for\n",
    "$x\\in\\mathbb{R}$, the function $p_{X}(x)=P\\{X=x\\}$ is called the\n",
    "*probability mass function* of $X$ . By the axioms of probability, a\n",
    "probability mass function $p_{X}$ satises\n",
    "$P\\{X\\in A\\}=\\sum_{x\\in A}p_{X}(x)$.\n",
    "\n",
    "**Definition**. Let $X$ be a continuous random variable with\n",
    "distribution function $F(t)=P\\{X\\leq t\\}$. Suppose that there exists a\n",
    "nonnegative, integrable function $f:\\mathbb{R}\\rightarrow[0,\\infty)$, or\n",
    "sometimes $f_{X}$ , such that $$F(x)=\\int_{-\\infty}^{x}f(y)dy$$ Then the\n",
    "function $f$ is called the *probability density function* of $X$ . We\n",
    "now have that for any $A\\subset\\mathbb{R}$ (or, more precisely, for any\n",
    "$A\\in\\digamma$ ), $$P\\{X\\in A\\}=\\int_{A}f_{X}(x)dx$$\n",
    "\n",
    "Bestiary of Distributions\n",
    "-------------------------\n",
    "![Imgur](http://i.imgur.com/ozle3EX.png)\n",
    "![Imgur](http://i.imgur.com/ty69A2m.png)\n",
    "![Relationships among probability distributions.](figs/0_home_dli_Dropbox_Notes_figure_pasted12.png)\n",
    "\n",
    "### Discrete models\n",
    "#### Binomial\n",
    "\n",
    "$f(x)=\\left(_{x}^{N}\\right)p^{x}(1-p)^{N-x}$, mean$=Np$, variance\n",
    "$=Np(1-p)$, depends on the number of samples per trail $N$. $N$\n",
    "increases, variance increases but the coefficient of variation (cv)\n",
    "$\\sqrt{Np(1-p)}/(Np)=\\sqrt{(1-p)/(Np)}$ decreases.\n",
    "\n",
    "You should only use the binomial in fitting data when there is an upper\n",
    "limit to the number of possible successes.\n",
    "\n",
    "When $N$ is large and $p$ isnt too close to 0 or 1 (i.e. when $Np$ is\n",
    "large, typically $Np\\geq10,\\,Nq\\geq10$), then the binomial distribution\n",
    "is approximately **normal**. When $N$ is large and $p$ is small, so that\n",
    "the probability of getting $N$ successes is small, the binomial\n",
    "approaches the **Poisson** distribution.\n",
    "\n",
    "![Binomial distribution.](figs/1_home_dli_Dropbox_Notes_figure_pasted3.png)\n",
    "\n",
    "#### Poisson\n",
    "\n",
    "The Poisson distribution gives the distribution of the number of\n",
    "individuals, arrivals, events, counts, etc., in a given time/space/unit\n",
    "of counting effort if each event is independent of all the others. The\n",
    "most common definition of the Poisson has only one parameter, the\n",
    "average density or arrival rate, $\\lambda$, *which equals the expected\n",
    "number of counts in a sampling unit*. An alternative parameterization\n",
    "gives a density per unit sampling effort and then specifies the mean as\n",
    "the product of the density per sampling effort $r$ times the sampling\n",
    "effort $t$, $\\lambda=rt$. This parameterization emphasizes that even\n",
    "when the population density is constant, you can change the Poisson\n",
    "distribution of counts by sampling more extensively for longer times or\n",
    "over larger quadrats.\n",
    "\n",
    "The Poisson distribution has no upper limit, although values much larger\n",
    "than the mean value are highly improbable. This characteristic provides\n",
    "*a rule for choosing between the binomial and Poisson*. If you expect to\n",
    "observe a ceiling on the number of counts, you should use the binomial;\n",
    "if you expect the number of counts to be effectively unlimited, even if\n",
    "it is theoretically bounded (e.g. there cant really be an infinite\n",
    "number of plants in your sampling quadrat), use the Poisson.\n",
    "\n",
    "The Poisson distribution *only makes sense for count data*. For\n",
    "$\\lambda<1$ the Poissons mode is at zero. When the expected number of\n",
    "counts gets large (e.g. $\\lambda>10$) the Poisson becomes approximately\n",
    "normal.\n",
    "\n",
    "![Poisson distribution.](figs/2_home_dli_Dropbox_Notes_figure_pasted4.png)\n",
    "\n",
    "#### Negative binomial\n",
    "\n",
    "Most probability books derive the negative binomial distribution from a\n",
    "series of independent binary (heads/tails, black/white, male/female,\n",
    "yes/no) trials that all have the same probability of success, like the\n",
    "binomial distribution. Rather than count the number of successes\n",
    "obtained in a fixed number of trials, which would result in a binomial\n",
    "distribution, the negative binomial counts ***the number of failures***\n",
    "before a predetermined number of successes occurs.\n",
    "\n",
    "This failure-process parameterization is only occasionally useful in\n",
    "ecological modeling. Ecologists use the negative binomial because it is\n",
    "discrete, like the Poisson, *but its variance can be larger than its\n",
    "mean* (i.e. it can be *overdispersed*). ***Thus, its a good\n",
    "phenomenological description of a patchy or clustered distribution with\n",
    "no intrinsic upper limit that has more variance than the Poisson***.\n",
    "\n",
    "The ecological parameterization of the negative binomial replaces the\n",
    "parameters $p$ (probability of success per trial: `prob` in R) and $n$\n",
    "(number of successes before you stop counting failures: `size` in R) with $\\mu=n(1-p)/p$,\n",
    "the mean number of failures expected (or of counts in a sample: \\texttt{mu}\n",
    "in R), and $k$, which is typically called an *overdispersion parameter*.\n",
    "(i.e. $p\\rightarrow\\mu,\\:n\\rightarrow k$). Confusingly, $k$ is also\n",
    "called `size` in R, because it is mathematically equivalent to $n$ in\n",
    "the failure-process parameterization.\n",
    "\n",
    "The overdispersion parameter measures the amount of clustering, or\n",
    "aggregation, or heterogeneity, in the data: *a smaller $k$ means more\n",
    "heterogeneity*. The variance of the negative binomial distribution is\n",
    "$\\mu+\\mu^{2}/k$, and *so as $k$ becomes large the variance approaches\n",
    "the mean and the distribution approaches the Poisson distribution.* For\n",
    "$k>10$, the negative binomial is hard to tell from a Poisson\n",
    "distribution, but $k$ is often less than 1 in ecological applications.\n",
    "\n",
    "Specifically, you can get a negative binomial distribution as the result\n",
    "of a Poisson sampling process where the rate $\\lambda$ itself varies. If\n",
    "the distribution of $\\lambda$ is a gamma distribution with shape\n",
    "parameter $k$ and mean $\\mu$, and $x$ is Poisson-distributed with mean\n",
    "$\\lambda$, then the distribution of $x$ be a negative binomial\n",
    "distribution with mean $\\mu$ and overdispersion parameter $k$ (May,\n",
    "1978; Hilborn and Mangel, 1997). In this case, the negative binomial\n",
    "reflects unmeasured (random) variability in the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in rgamma(1000, shape = k, scale = mu/k): object 'k' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in rgamma(1000, shape = k, scale = mu/k): object 'k' not found\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in rpois(1000, lambda): object 'lambda' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in rpois(1000, lambda): object 'lambda' not found\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in dnbinom(0:max(z), mu = mu, size = k): object 'z' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in dnbinom(0:max(z), mu = mu, size = k): object 'z' not found\n"
     ]
    }
   ],
   "source": [
    "lambda <- rgamma(1000, shape = k, scale = mu/k) # scale=mean/shape\n",
    "z <- rpois(1000, lambda)\n",
    "p2 = dnbinom(0:max(z), mu = mu, size = k)\n",
    "# z = p2 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative binomial distributions can also result from a homogeneous\n",
    "birth-death process, births and deaths (and immigrations) occurring at\n",
    "random in continuous time. Samples from a population that starts from 0\n",
    "at time $t=0$, with immigration rate $i$, birth rate $b$, and death rate\n",
    "$d$ will be negative binomially distributed with parameters\n",
    "$\\mu=i/(b-d)(e^{(b-d)t}-1)$ and $k=i/b$ (Bailey, 1964, p. 99).\n",
    "\n",
    "Several different ecological processes can often generate the same\n",
    "probability distribution. We can usually reason forward from knowledge\n",
    "of probable mechanisms operating in the field to plausible distributions\n",
    "for modeling data, but this many-to-one relationship suggests that it is\n",
    "unsafe to reason backwards from probability distributions to particular\n",
    "mechanisms that generate them.\n",
    "\n",
    "Rs default coin-flipping ($n=$`size`, $p=$`prob`) parameterization. In order to use the ecological ($\\mu=$`mu`, $k=$`size`) parameterization, you must name the mu parameter explicitly (e.g. `dnbinom(5,size=0.6,mu=1)`).\n",
    "\n",
    "![Negative binomial distribution Mean mu=2 in all cases ](figs/3_home_dli_Dropbox_Notes_figure_pasted5.png)\n",
    "\n",
    "#### Geometric\n",
    "\n",
    "The geometric distribution is the number of trials (with a constant\n",
    "probability of fail(1-pure) until you get a single failure: its a\n",
    "special case of the negative binomial, with $k$ or $n$ = 1.\n",
    "\n",
    "![Geometric distribution.](figs/4_home_dli_Dropbox_Notes_figure_pasted6.png)\n",
    "\n",
    "#### Beta-Binomial\n",
    "\n",
    "Just as one can compound the Poisson distribution with a Gamma to allow\n",
    "for heterogeneity in rates, producing a negative binomial, one can\n",
    "compound the binomial distribution with a Beta distribution to allow for\n",
    "heterogeneity in per-trial probability, producing a Beta-binomial\n",
    "distribution (Crowder, 1978; Reeve and Murdoch, 1985; Hatfield et al.,\n",
    "1996).\n",
    "\n",
    "The *most common* parameterization of the beta-binomial distribution\n",
    "uses the binomial parameter $N$ (trials per sample), plus two additional\n",
    "parameters $a$ and $b$ that describe the beta distribution of the\n",
    "per-trial probability. When $a=b=1$ the per-trial probability is equally\n",
    "likely to be any value between 0 and 1 (the mean is 0.5), and the\n",
    "beta-binomial gives a uniform (discrete) distribution between 0 and $N$\n",
    ". As $a+b$ increases, the variance of the underlying heterogeneity\n",
    "decreases and the beta-binomial converges to the *binomial*\n",
    "distribution. Morris (1997) suggests a different parameterization that\n",
    "uses an overdispersion parameter $\\theta$, like the $k$ parameter of the\n",
    "negative binomial distribution. In this case the parameters are $N$ ,\n",
    "the per-trial probability $p=a/(a+b)$, and $\\theta=a+b$. When $\\theta$\n",
    "is large (small overdispersion), the beta-binomial becomes binomial.\n",
    "When $\\theta$ is near zero (large overdispersion), the beta-binomial\n",
    "becomes U-shaped.\n",
    "\n",
    "![Beta-binomial distribution](figs/5_home_dli_Dropbox_Notes_figure_pasted7.png)\n",
    "Beta-binomial distribution. Number of trials ($N$) equals 10, average per-trial probability ($p$) equals 0.5 for all distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Distributions\n",
    "\n",
    "#### Uniform distribution\n",
    "\n",
    "The uniform distribution with limits a and b, denoted $U(a,b)$, has a\n",
    "constant probability density of $1/(b-a)$ for $a\\leq x\\leq b$ and zero\n",
    "probability elsewhere. The standard uniform, $U(0,1)$, is very commonly\n",
    "used as a building block for other distributions, but is surprisingly\n",
    "rarely used in ecology otherwise.\n",
    "\n",
    "#### Normal distribution\n",
    "\n",
    "#### Gamma distribution\n",
    "\n",
    "**The Gamma distribution is the distribution of** ***waiting times***\n",
    "**until a certain number of events take place.** For example,\n",
    "$Gamma(shape=3,scale=2)$ is the distribution of the length of time (in\n",
    "days) youd expect to have to wait for 3 deaths in a population, given\n",
    "that the average survival time is 2 days (mortality rate is 1/2 per\n",
    "day). The mean waiting time is 6 days = 3 deaths/(1/2 death per day).\n",
    "Gamma distributions with integer shape parameters are also called\n",
    "*Erlang* distributions. The Gamma distribution is still defined for\n",
    "non-integer (positive) shape parameters, but the simple description\n",
    "given above breaks down: how can you define the waiting time until 3.2\n",
    "events take place?\n",
    "\n",
    "For shape parameters $\\le$ 1, the Gamma has its mode at zero; for shape\n",
    "parameter = 1, the Gamma is equivalent to the exponential (see below).\n",
    "For shape parameter greater than 1, the Gamma has a peak (mode) at a\n",
    "value greater than zero; as the shape parameter increases, the Gamma\n",
    "distribution becomes more symmetrical and approaches the normal\n",
    "distribution. This behavior makes sense if you think of the Gamma as the\n",
    "distribution of the sum of independent, identically distributed waiting\n",
    "times, in which case it is governed by the Central Limit Theorem.\n",
    "\n",
    "The scale parameter (sometimes defined in terms of a rate parameter\n",
    "instead, 1/scale) just adjusts the mean of the Gamma by adjusting the\n",
    "waiting time per event; however, multiplying the waiting time by a\n",
    "constant to adjust its mean also changes the variance, so both the\n",
    "variance and the mean depend on the scale parameter.\n",
    "\n",
    "The Gamma distribution is less familiar than the normal, and new users\n",
    "of the Gamma often find it annoying that in the standard\n",
    "parameterization you cant adjust the mean independently of the variance.\n",
    "You could define a new set of parameters $m$ (mean) and $v$ (variance),\n",
    "with scale = $v/m$ and shape = $m^{2}/v$ but then you would find (unlike\n",
    "the normal distribution) the shape changing as you changed the variance.\n",
    "Nevertheless, the Gamma is extremely useful; it solves the problem that\n",
    "many researchers face when they have a continuous variable with *too\n",
    "much variance*, whose coefficient of variation is greater than about\n",
    "0.5. Modeling such data with a normal distribution leads to unrealistic\n",
    "negative values, which then have to be dealt with in some ad hoc way\n",
    "like truncating them or otherwise trying to ignore them. The Gamma is\n",
    "often a more realistic alternative.\n",
    "\n",
    "The Gamma is the continuous counterpart of the negative binomial, which\n",
    "is the discrete distribution of a number of trials (rather than length\n",
    "of time) until a certain number of events occur. Both the negative\n",
    "binomial and Gamma distributions are often generalized, however, in ways\n",
    "that dont necessarily make sense according to their simple mechanistic\n",
    "descriptions (e.g. a Gamma distribution with a shape parameter of 2.3\n",
    "corresponds to the distribution of waiting times until 2.3 events occur\n",
    ". . . ).\n",
    "\n",
    "The Gamma and negative binomial are both commonly used\n",
    "phenomenologically, as skewed or overdispersed versions of the Poisson\n",
    "or normal distributions, rather than for their mechanistic descriptions.\n",
    "***The Gamma is less widely used than the negative binomial*** **because\n",
    "the negative binomial replaces the Poisson, which is restricted to a\n",
    "particular variance, while the Gamma replaces the normal, which can have\n",
    "any variance. Thus you might use the negative binomial for any discrete\n",
    "distribution with variance \\> mean, while you wouldnt need a Gamma\n",
    "distribution unless the distribution you were trying to match was skewed\n",
    "to the right.**\n",
    "\n",
    "![Gamma distribution](figs/6_home_dli_Dropbox_Notes_figure_pasted8.png)\n",
    "\n",
    "#### Exponential\n",
    "\n",
    "*The exponential distribution describes the distribution of waiting\n",
    "times for a single event to happen, given that there is a constant\n",
    "probability per unit time that it will happen. It is the continuous\n",
    "counterpart of the geometric distribution and a special case (for shape\n",
    "parameter$=1$) of the Gamma distribution*. It can be useful both\n",
    "mechanistically, as a distribution of inter-event times or lifetimes, or\n",
    "phenomenologically, for any continuous distribution that has highest\n",
    "probability for zero or small values.\n",
    "\n",
    "![Exponential distribution.](figs/7_home_dli_Dropbox_Notes_figure_pasted9.png)\n",
    "\n",
    "#### Beta\n",
    "\n",
    "The beta distribution is a continuous distribution closely related to\n",
    "the binomial distribution. *The beta distribution is the only standard\n",
    "continuous distribution (besides the uniform distribution) with a finite\n",
    "range, from 0 to 1.* **The beta distribution is the inferred\n",
    "distribution of the probability of success in a binomial trial with\n",
    "$a-1$ observed successes and $b-1$ observed failures.** When $a=b$ the\n",
    "distribution is symmetric around $x=0.5$, when $a<b$ the peak shifts\n",
    "toward zero, and when $a>b$ it shifts toward 1. *With $a=b=1$, the\n",
    "distribution is $U(0,1)$*. As $a+b$ (equivalent to the total number of\n",
    "trials$+2$) gets larger, the distribution becomes more peaked. For $a$\n",
    "or $b$ less than 1, the mechanistic description stops making sense (how\n",
    "can you have fewer than zero trials?), but the distribution is still\n",
    "well-defined, and when a and b are both between 0 and 1 it becomes\n",
    "U-shaped it has peaks at $p=0$ and $p=1$.\n",
    "\n",
    "The beta distribution is obviously good for modeling probabilities or\n",
    "proportions. It can also be useful for modeling continuous distributions\n",
    "with peaks at both ends, although in some cases a finite mixture model\n",
    "may be more appropriate. The beta distribution is also useful whenever\n",
    "you have to define a continuous distribution on a finite range, as it is\n",
    "the only such standard continuous distribution. Its easy to rescale the\n",
    "distribution so that it applies over some other finite range instead of\n",
    "from 0 to 1: for example, Tiwari et al. (2005) used the beta\n",
    "distribution to describe the distribution of turtles on a beach, so the\n",
    "range would extend from 0 to the length of the beach.\n",
    "\n",
    "![Beta distribution](figs/8_home_dli_Dropbox_Notes_figure_pasted10.png)\n",
    "\n",
    "#### Lognormal\n",
    "\n",
    "Its mechanistic justification is like the normal distribution (the\n",
    "Central Limit Theorem), but for the product of many independent,\n",
    "identical variates rather than their sum. Just as taking logarithms\n",
    "converts products into sums, taking the logarithm of a lognormally\n",
    "distributed variablewhich might result from the product of independent\n",
    "variablesconverts it it into a normally distributed variable resulting\n",
    "from the sum of the logarithms of those independent variables. The best\n",
    "example of this mechanism is the distribution of the sizes of\n",
    "individuals or populations that grow exponentially, with a per capita\n",
    "growth rate that varies randomly over time. At each time step (daily,\n",
    "yearly, etc.), the current size is multiplied by the randomly chosen\n",
    "growth increment, so the final size (when measured) is the product of\n",
    "the initial size and all of the random growth increments.\n",
    "\n",
    "The log-normal is also used phenomenologically in some of the same\n",
    "situations where a Gamma distribution also fits: continuous, positive\n",
    "distributions with long tails or variance much greater than the mean\n",
    "(McGill et al., 2006). Like the distinction between a Michaelis-Menten\n",
    "and a saturating exponential, you may not be able to tell the difference\n",
    "between a lognormal and a Gamma without large amounts of data. Use the\n",
    "one that is more convenient, or that corresponds to a more plausible\n",
    "mechanism for your data.\n",
    "\n",
    "a=20;b=1;k=5\n",
    "![Lognormal distribution](figs/9_home_dli_Dropbox_Notes_figure_pasted11.png \"fig:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 5: Stochastic Simulation and Power Analysis\n",
    "========================================\n",
    "\n",
    "Simulation is sometimes called *forward* modeling, to emphasize that you\n",
    "pick a model and parameters and work forward to predict patterns in the\n",
    "data. Parameter estimation, or *inverse* modeling (the main focus of\n",
    "this book), starts from the data and works backward to choose a model\n",
    "and estimate parameters.\n",
    "\n",
    "Stochastic Simulation\n",
    "---------------------\n",
    "\n",
    "### Simple examples\n",
    "\n",
    ": $Y\\sim Normal(a+bx,\\,\\sigma^{2})$, or can also be written as\n",
    "$y_{i}=a+bx_{i}+\\varepsilon_{i},\\,\\varepsilon_{i}\\sim N(0,\\sigma^{2})$,\n",
    "specifying that the $i$ th value of $Y$ , $y_{i}$ , is equal to\n",
    "$a+bx_{i}$ plus a normally distributed error term with mean zero. But\n",
    "the first form is more general. Normally distributed error is one of the\n",
    "few kinds that can simply be added onto the deterministic model in the\n",
    "second way.\n",
    "\n",
    "```\n",
    "x=1:20;a=2;b=1 \n",
    "y_det=a+b*x \n",
    "y=rnorm(20, mean=y_det, sd=2)\n",
    "```\n",
    "\n",
    "A simple simulation uses hyperbolic functions ($y=ab/(b+x)$) with\n",
    "negative binomial error: in symbols, $Y\\sim NegBin(\\mu=ab/(b+x),\\,k)$.\n",
    "The function is parameterized so that a is the intercept term (when\n",
    "$x=0,y=ab/b=a$). *This simulation might represent the decreasing\n",
    "fecundity of two different species with increasing population density;\n",
    "the hyperbolic function is a natural expression of the decreasing\n",
    "quantity of a limiting resource per individual.* In this case, we cannot\n",
    "express the model as the deterministic function plus error. Instead, we\n",
    "have to incorporate the deterministic model as a control on one of the\n",
    "parameters of the error distributionin this case, the mean $\\mu$.\n",
    "(Although the negative binomial is a discrete distribution, its\n",
    "parameters $\\mu$ and $k$ are continuous.)\n",
    "\n",
    "```\n",
    "a <- 20\n",
    "b <- 1\n",
    "k <- 5\n",
    "x <- runif(50, min = 0, max = 5)\n",
    "y_det <- a * b/(b + x)\n",
    "y <- rnbinom(50, mu = y_det, size = k)\n",
    "```\n",
    "\n",
    "Chapter 6: Likelihood and all that\n",
    "=======================\n",
    "\n",
    "Previous chapters have introduced all the ingredients you need to define\n",
    "a model mathematical functions to describe the deterministic patterns\n",
    "and probability distributions to describe the stochastic patterns and\n",
    "shown how to use these ingredients to simulate simple ecological\n",
    "systems. However, you need to learn not only how to construct models but\n",
    "also how to estimate parameters from data, and how to test models\n",
    "against each other.\n",
    "\n",
    "In general, to estimate the parameters of a model we have to find the\n",
    "parameters that make that model fit the data best. To compare among\n",
    "models we have to figure out which one fits the data best, and decide if\n",
    "one or more models fit sufficiently much better than the rest that we\n",
    "can declare them the winners. Our goodness-of-fit metrics will be based\n",
    "on the ***likelihood, the probability of seeing the data we actually\n",
    "collected given a particular model*** which in this case will mean both\n",
    "the general form of the model and the specific parameter values.\n",
    "\n",
    "Parameter estimation: Single distributions\n",
    "------------------------------------------\n",
    "\n",
    "### Maximum likelihood\n",
    "\n",
    "We want the *maximum likelihood estimates* of the parameters those\n",
    "parameter values that make the observed data most likely to have\n",
    "happened. *i.i.d* data, so the joint likelohood of the wholde data set\n",
    "is the product of the likelihood of each individual observation. For\n",
    "mathmatical convience, we take the logarithm of the likelihood. Since\n",
    "the logarithm is a monotonically increasing function, the maximum\n",
    "log-likelihood estimate is the same as the maximum likelihood estimate.\n",
    "Actually, it is conventional to ***minimize*** the *negative\n",
    "log-likelihood* rather than maximizing the log-likelihood. For\n",
    "continuous probability distributions, we compute the probability density\n",
    "of observing the data rather than the probability itself. Since we are\n",
    "interested in relative (log)likelihoods, not the absolute probability of\n",
    "observing the data, we can ignore the distinction between the density\n",
    "($P(x)$) and the probability (which includes a term for the measurement\n",
    "precision: $P(x)dx$).\n",
    "\n",
    "#### Binomial maximum likelihood\n",
    "\n",
    "Analytical approach:\n",
    "$$l=\\prod_{i=1}^{n}\\left(_{k_{i}}^{N}\\right)p^{k_{i}}(1-p)^{N-k_{i}}$$\n",
    "$$L=log(l)=\\sum_{i=1}^{n}\\left(log\\left(_{k_{i}}^{N}\\right)+k_{i}log(p)+(N-k_{i})log(1-p)\\right)$$\n",
    "then solve $\\frac{dL}{dp}=0$, get\n",
    "$\\hat{p}=\\frac{\\sum_{i=1}^{n}k_{i}}{nN}$.\n",
    "\n",
    "Numerics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(emdbook)\n",
    "data(ReedfrogPred)\n",
    "binomNLL1=function(p,k,N){-sum(dbinom(k, prob=p, size=N, log=T))}\n",
    "x=subset(ReedfrogPred, pred==\"pred\" & density ==10 & size==\"small\")\n",
    "k=x$surv\n",
    "opt1=optim(fn=binomNLL1, par=c(p=0.5), N=10, k=k, method=\"BFGS\")\n",
    "# use method = \"BFGS\" for a single-parameter fit.\n",
    "opt1$par # the estimated value of the probability\n",
    "exp(-opt1$value) # the estimated value of max. likelihood. \n",
    "#But usually we only care about the relative value. \n",
    "library(bbmle)\n",
    "m1=mle2(minuslogl=binomNLL1, start=list(p=0.5), data=list(N=10, k=k))\n",
    "m1\n",
    "mle2(k~dbinom(prob=p, size=10), start=list(p=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Gamma likelihhod\n",
    "\n",
    "The likelihood equation for Gamma-distributed data is hard to maximize\n",
    "analytically, so well go straight to a numerical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data(MyxoTiter_sum)\n",
    "myxdat=subset(MyxoTiter_sum, grade==1)\n",
    "gammaNLL1=function(shape, scale) {-sum(dgamma(myxdat$titer, shape=shape,scale=scale,log=T))}\n",
    "# starting paramters for gamma distribution are hard to find.\n",
    "# we can use those from the mothod of moments\n",
    "gm=mean(myxdat$titer)\n",
    "gvm=var(myxdat$titer)/mean(myxdat$titer)\n",
    "m3=mle2(gammaNLL1, start=list(shape=gm/gvm, scale=gvm))\n",
    "m3\n",
    "mle2(myxdat$titer ~ dgamma(shape, scale = scale),start = list(shape = gm/gvm, scale = gvm),data=myxdat)\n",
    "fitdistr(myxdat$titer, \"gamma\") # report the rate = 1/scale\n",
    "# fitdistr from MASS package, good for a single paramter fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Bayesian analysis\n",
    "\n",
    "Bayesian estimation also uses the likelihood, but it differs in two ways\n",
    "from maximum likelihood analysis. **First**, we combine the likelihood\n",
    "with *a prior probability distribution* in order to determine a\n",
    "posterior probability distribution. **Second**, we often report the\n",
    "*mean* of the posterior distribution rather than its mode (which would\n",
    "equal the MLE if we were using a completely uninformative or flat\n",
    "prior). Unlike the mode, which reflects only local information about the\n",
    "peak of the distribution, the mean incorporates the entire pattern of\n",
    "the distribution, so it can be harder to compute.\n",
    "\n",
    "#### Binomial distribution conjugate priors: Beta distribution\n",
    "\n",
    "Conjugate priors also allow us to interpret the strength of the prior in\n",
    "simple ways. For example, the conjugate prior of the binomial likelihood\n",
    "that we used for the tadpole predation data is the Beta distribution. If\n",
    "we pick a Beta prior with shape parameters $a$ and $b$, and if our data\n",
    "include a total of $\\sum k$ successes (predation events) and $nN-\\sum k$\n",
    "failures (surviving tadpoles) out of a total of $nN$ trials (exposed\n",
    "tadpoles), the posterior distribution is a Beta distribution with shape\n",
    "parameters $a+\\sum k$, that is ($a+successes$) and $b+(nN-\\sum k)$, that\n",
    "is $b+failures$. If we interpret $a-1$ as the total number of previously\n",
    "observed successes and $b-1$ as the number of previously observed\n",
    "failures, then the new distribution just combines the total number of\n",
    "successes and failures in the complete (prior plus current) data set.\n",
    "When $a=b=1$, the Beta distribution is flat, corresponding to no prior\n",
    "information ($a-1=b-1=0$). When $a=b\\rightarrow0$, we get a very\n",
    "peculiar prior distribution with infinite spikes at 0 and 1. As $a$ and\n",
    "$b$ increase, the prior distribution gains more information and becomes\n",
    "peaked. We can also see that, as far as a Bayesian is concerned, it\n",
    "doesnt matter how we divide our experiments up. Many small experiments,\n",
    "aggregated with successive uses of Bayes Rule, give the same information\n",
    "as one big experiment (provided of course that there is no variation in\n",
    "pertrial probability among sets of observations, which we have assumed\n",
    "in our statistical model for both the likelihood and the Bayesian\n",
    "analysis). To sum up:\n",
    "\n",
    "1.  The data: $X\\sim Bin(n,p)$,\n",
    "    $f(x|p)=\\frac{n!}{x!(n-x)!}p^{x}(1-p)^{n-x}$\n",
    "\n",
    "2.  The prior: $p\\sim Beta(a,b)$,\n",
    "    $\\pi(p)=\\frac{p^{a-1}(1-p)^{b-1}}{Beta(a,b)}$,\n",
    "    $E(p)=\\frac{a}{a+b},Var(p)=\\frac{ab}{(a+b)^{2}(a+b+1)}$. If both\n",
    "    $a,b$ are integers, then $Beta(a,b)=\\frac{(a-1)!(b-1)!}{(a+b-1)!}$.\n",
    "\n",
    "3.  The posterior: $p|x\\sim Beta(a+x,b+n-x)$\n",
    "\n",
    "#### Poisson distribution conjugate priors: Gamma distribution\n",
    "\n",
    "If our data were Poisson, we could use a conjugate prior Gamma\n",
    "distribution with shape $\\alpha$ and scale $s$ and interpret the\n",
    "parameters as $\\alpha=$total counts in previous observations and\n",
    "$1/s=$number of previous observations. Then if we observed $C$ counts in\n",
    "our data, the posterior would be a Gamma distribution with\n",
    "$\\alpha'=\\alpha+C$, $1/s'=1/s+1$. Sum up:\n",
    "\n",
    "1.  The data: $X\\sim Poisson(\\lambda)$,\n",
    "    $f(x|\\lambda)=\\frac{\\lambda^{x}e^{-\\lambda}}{x!}$\n",
    "\n",
    "2.  The prior: $\\lambda\\sim Gamma(shape=\\alpha,rate=\\frac{1}{s})$. Some\n",
    "    define the Gamma distribution in terms of $scale=s$.\n",
    "    $\\pi(\\lambda)=\\frac{(1/s)^{\\alpha}}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1}e^{-\\beta\\lambda}$.\n",
    "    $\\Gamma(\\alpha)$ is the Gamma function.\n",
    "    $E(\\lambda)=\\alpha s,\\,Var(\\lambda)=\\alpha s^{2}$. If $\\alpha$ is a\n",
    "    positive integer, then $\\Gamma(\\alpha)=(\\alpha-1)!$.\n",
    "\n",
    "3.  The posterior:\n",
    "    $\\lambda|x_{1},\\cdots,x_{n}\\sim Gamma(\\alpha+\\sum x_{i},\\frac{1}{s}+n)$.\n",
    "    In the above example, $x_{1}=C,n=1$.\n",
    "\n",
    "#### Normal distribution conjugate priors\n",
    "\n",
    "-   Normal with known variance, unkown mean\n",
    "\n",
    "    1.  The data: $X\\sim N(\\mu,\\sigma^{2}=1)$, we know $\\sigma^{2}$, say\n",
    "        $\\sigma^{2}=1$.\n",
    "\n",
    "    2.  The prior: $\\mu\\sim N(\\theta=3,\\tau^{2}=4)$.\n",
    "\n",
    "    3.  The posterior:\n",
    "        $\\mu|x_{1},\\cdots,x_{n}=N\\left(\\frac{\\frac{n\\overline{X}}{\\sigma^{2}}+\\frac{\\theta}{\\tau^{2}}}{\\frac{n}{\\sigma^{2}}+\\frac{1}{\\tau^{2}}},\\frac{1}{\\frac{n}{\\sigma^{2}}+\\frac{1}{\\tau^{2}}}\\right)$\n",
    "\n",
    "-   Normal with known mean, unkown variance\n",
    "\n",
    "    1.  The data: $X\\sim N(\\mu=1,\\sigma^{2})$.\n",
    "\n",
    "    2.  The prior of precision:\n",
    "        $\\varnothing=\\frac{1}{\\sigma^{2}}=Gamma(shape=\\alpha,rate=\\beta)$\n",
    "\n",
    "    3.  The posterior:\n",
    "        $\\varnothing|x_{1},\\cdots,x_{n}=Gamma(shape=\\alpha+\\frac{n}{2},\\,rate=\\beta+\\sum(x_{i}-\\mu)^{2}/2$\n",
    "\n",
    "-   Normal with unkown mean, unkown variance\n",
    "\n",
    "    1.  The data: $X\\sim N(\\mu,\\sigma^{2})$.\n",
    "\n",
    "    2.  The prior: $\\mu|\\sigma^{2}\\sim N(\\mu_{0},\\sigma^{2}/\\lambda)$.\n",
    "        Note that this prior for $\\mu$ depends on the unknown\n",
    "        $\\sigma^{2}$ . $\\sigma^{2}$\\~ Inverse $\\chi^{2}$ with df $v$ and\n",
    "        scale $\\sigma_{0}^{2}$ .\n",
    "\n",
    "    3.  $\\mu|$$x_{1},\\cdots,x_{n}\\sim t$ with df $v+n$. The “center” of\n",
    "        the $t$ distribution is\n",
    "        $\\frac{\\lambda}{\\lambda+n}\\mu_{0}+\\frac{n}{\\lambda+n}\\bar{x}$.\n",
    "        The “scale” of the $t$ posterior is a weighted average of\n",
    "        $\\sigma_{0}^{2}$, the sample variance $s^{2}$, and\n",
    "        $(\\bar{x}-\\mu_{0})^{2}$.\n",
    "\n",
    "#### Gamma distribution: nonconjugate prior.\n",
    "\n",
    "Unfortunately simple conjugate priors arent always available, and we\n",
    "often have to resort to numerical integration to evaluate Bayes Rule.\n",
    "Just plotting the numerator of Bayes Rule, ($prior(p)\\times L(p)$), is\n",
    "easy: for anything else, we need to integrate (or use summation to\n",
    "approximate an integral).\n",
    "\n",
    "**Bayesians often use the Gamma as a prior distribution for parameters\n",
    "that must be positive**. Using a small shape parameter gives the\n",
    "distribution a large variance (corresponding to little prior\n",
    "information) and means that the distribution will be peaked at small\n",
    "values but is likely to be flat over the range of interest. Finally, the\n",
    "scale is usually set large enough to make the mean of the parameter (=\n",
    "shape · scale) reasonable.\n",
    "\n",
    "For a Gamma distribution with shape$=a$ and scale$=s$. Example:\n",
    "Prior(a)$\\sim$Gamma($shape=0.01,scale=100$);\n",
    "Prior(s)=Gamma($shape=0.1,scale=10$);\n",
    "Prior(a,s)$=$Prior(a)$\\times$Prior(s).\n",
    "$$Posterior(a,s)=\\frac{Prior(a,s)\\times\\mathcal{L}(a,s)}{\\iint Prior(a,s)\\times\\mathcal{L}(a,s)da\\,ds}$$\n",
    "$$\\bar{a}=\\iint Prior(a,s)\\times\\mathcal{L}(a,s)\\,a\\,da\\,ds;\\bar{s}=\\iint Prior(a,s)\\times\\mathcal{L}(a,s)\\,s\\,da\\,ds$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior.as <- function(a, s) {\n",
    "dgammaa(a, shape = 0.01, scale = 100) * dgamma(s, shape = 0.1, scale = 10)\n",
    "}\n",
    "unscaled.posterior <- function(a, s) {\n",
    "prior.as(a, s) * exp(-gammaNLL1(shape = a, scale = s))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use integrate (for 1-dimensional integrals) or adapt (in the adapt\n",
    "package; for multi-dimensional integrals) to do the integration. More\n",
    "crudely, we can approximate the integral by a sum, calculating values of\n",
    "the integrand for discrete values. However, integrating probabilities is\n",
    "tricky for two reasons. (1) Prior probabilities and likelihoods are\n",
    "often tiny for some parameter values, leading to roundoff error; tricks\n",
    "like calculating log-probabilities for the prior and likelihood, adding,\n",
    "and then exponentiating can help. (2) You must pick the number and range\n",
    "of points at which to evaluate the integral carefully. Too coarse a grid\n",
    "leads to approximation error, which may be severe if the function has\n",
    "sharp peaks. Too small a range, or the wrong range, can miss important\n",
    "parts of the surface. In practice, brute-force numerical integration is\n",
    "no longer feasible with models with more than about two parameters. The\n",
    "only practical alternatives are Markov chain Monte Carlo approaches.\n",
    "\n",
    "Estimation for more complex functions\n",
    "-------------------------------------\n",
    "\n",
    "### Maximum likelihood\n",
    "\n",
    "```\n",
    "mle2(titer ~ dgamma(shape, scale = a * day * exp(-b * day)/shape), \n",
    "    start = list(a = 1, b = 0.2, shape = 50), \n",
    "    data = myxdat, method = \"Nelder-Mead\")\n",
    "```\n",
    "\n",
    "### Bayesian analysis\n",
    "\n",
    "MCMC\\. BUGSs modeling language is similar but not identical to R. For\n",
    "example, BUGS requires you to use `<-` instead of `=` for assignments.\n",
    "BUGS uses shape and rate parameters to define the Gamma distribution\n",
    "rather than shape and scale parameters: differences in parameterization\n",
    "are some of the most important differences between the BUGS and R\n",
    "languages. tilde (`~`) means is distributed as.\n",
    "\n",
    "Likelihood surfaces, profiles, and confidence intervals\n",
    "-------------------------------------------------------\n",
    "\n",
    "### Frequentist analysis: likelihood curves and profiles\n",
    "\n",
    "The most basic tool for understanding how likelihood depends on one or\n",
    "more parameters is the *likelihood curve* or *likelihood surface*, which\n",
    "is just the likelihood plotted as a function of parameter values. By\n",
    "convention, we plot the negative log-likelihood rather than\n",
    "log-likelihood, so the best estimate is a minimum rather than a maximum.\n",
    "On a negative log-likelihood curve or surface, higher points represent\n",
    "worse fits.\n",
    "\n",
    "If we want to deal with models with more than two parameters, or if we\n",
    "want to analyze a single parameter at a time, we have to find a way to\n",
    "isolate the effects of one or more parameters while still accounting for\n",
    "the rest. A simple, but usually **wrong**, way of doing this is to\n",
    "calculate a likelihood *slice*, fixing the values of all but one\n",
    "parameter (usually at their maximum likelihood estimates) and then\n",
    "calculating the likelihood for a range of values of the focal parameter.\n",
    "\n",
    "Instead, we calculate likelihood *profiles*, which represent ridgelines\n",
    "in parameter space showing the minimum negative log-likelihoods for\n",
    "particular values of a single parameter. To calculate a likelihood\n",
    "profile for a focal parameter, we have to set the focal parameter in\n",
    "turn to a range of values, and for each value optimize the likelihood\n",
    "with respect to all of the other parameters.\n",
    "\n",
    "Slices are always steeper than profiles, because they dont allow the\n",
    "other parameters to adjust to changes in the focal parameter.\n",
    "\n",
    "#### The likelihood ratio test\n",
    "\n",
    "Take some likelihood function $L(p_{1},p_{2},...,p_{n})$, and find the\n",
    "overall best (maximum likelihood) value,\n",
    "$L_{abs}=L(\\hat{p}_{1},\\hat{p}_{2},...,\\hat{p}_{n})$ (abs stands for\n",
    "absolute). Now fix some of the parameters (say $p_{1}...p_{r}$ ) to\n",
    "specific values ($p_{1}^{\\ast},...p_{r}^{\\ast}$), and maximize with\n",
    "respect to the remaining parameters to get\n",
    "$L_{restr}=L(p_{1}^{\\ast},...p_{r}^{\\ast},\\hat{p}_{r+1},...,\\hat{p}_{n})$\n",
    "(restr stands for restricted, sometimes also called a *reduced or\n",
    "nested* model). The Likelihood Ratio Test says that the distribution of\n",
    "twice the negative log of the likelihood ratio,\n",
    "$-2log(L_{restr}/L_{abs})$, called the *deviance*, is approximately\n",
    "$\\chi^{2}$ (chi-squared) distribution with $r$ degrees of freedom.\n",
    "$$2\\left(-\\log\\mathcal{L}_{restr}-(-\\log\\mathcal{L}_{abs})\\right)\\sim\\chi_{r}^{2}$$\n",
    "The definition of the LRT echoes the definition of the likelihood\n",
    "profile, where we fix one parameter and maximize the likelihood/minimize\n",
    "the negative log-likelihood with respect to all the other parameters:\n",
    "$r=1$ in the definition above. Thus, for univariate confidence limits we\n",
    "cut off the likelihood profile at (min. neg. log. likelihood +\n",
    "$\\chi_{1}^{2}(1-\\alpha)/2$), where $\\alpha$ is our chosen confidence\n",
    "level (0.95, 0.99, etc.). (The cutoff is a one-tailed test, since we are\n",
    "looking only at differences in likelihood that are larger than expected\n",
    "under the null hypothesis.)\n",
    "\n",
    "`R` can compute profiles and profile confidence limits automatically.\n",
    "Given an `mle2` fit `m`, `profile(m)` will compute a likelihood profile\n",
    "and `confint(m)` will compute profile confidence limits.\n",
    "`plot(profile(m2))` will plot the profile, square-root transformed so\n",
    "that a quadratic profile will appear V-shaped (or linear if you specify\n",
    "`absval=FALSE`). This transformation makes it easier to see whether the\n",
    "profile is quadratic, since its easier to see whether a line is straight\n",
    "than it is to see whether its quadratic.\n",
    "\n",
    "The LRT is only correct asymptotically, for large data sets. For small\n",
    "data sets it is an approximation, although one that people use very\n",
    "freely. The other limitation of the LRT that frequently arises, although\n",
    "it is often ignored, is that it only works when the best estimate of the\n",
    "parameter is not on the edge of its allowable range (Pinheiro and Bates,\n",
    "2000). For example, if you are fitting an exponential model $y=\\exp(rx)$\n",
    "that must be decreasing, so that $r\\leq0$, and your best estimate of $r$\n",
    "is equal to 0, then the LRT estimate for the upper bound of the\n",
    "confidence limit is not technically correct.\n",
    "\n",
    "### Bayesian approach: posterior distribution and marginal distributions\n",
    "\n",
    "Instead of drawing likelihood curves, Bayesians draw the posterior\n",
    "distribution (proportional to prior L). Instead of calculating\n",
    "confidence limits using the (frequentist) LRT, they define the *credible\n",
    "interval*, which is the region in the center of the distribution\n",
    "containing 95% (or some other standard proportion) of the probability of\n",
    "the distribution, *bounded by values on either side that have the*\n",
    "***same*** *probability* (or probability density). Technically, the\n",
    "credible interval is the interval $[x_{1},x_{2}]$ such that\n",
    "$P(x_{1})=P(x_{2})$ and $C(x_{2})-C(x_{1})=1-\\alpha$, where P is the\n",
    "probability density and C is the cumulative density. The credible\n",
    "interval is slightly different from the frequentist confidence interval,\n",
    "which is defined as $[x_{1},x_{2}]$ such that $C(x_{1})=\\alpha/2$ and\n",
    "$C(x_{2})=1-\\alpha/2$.\n",
    "\n",
    "The *marginal probability density* is the Bayesian analogue of the\n",
    "likelihood profile. Where frequentists use likelihood profiles to make\n",
    "inferences about a single parameter while taking the effects of the\n",
    "other parameters into account, Bayesians use the marginal posterior\n",
    "probability density, the overall probability for a particular value of a\n",
    "focal parameter integrated over all the other parameters.\n",
    "\n",
    "Confidence intervals for comples models: quadratic approximation\n",
    "----------------------------------------------------------------\n",
    "\n",
    "Comparing models\n",
    "----------------\n",
    "\n",
    "Parsimony (sometimes called Occams razor) is a general argument for\n",
    "choosing simpler models even though we know the world is complex. All\n",
    "other things being equal, we should prefer a simpler model to a more\n",
    "complex one especially when the data dont tell a clear story. Model\n",
    "selection approaches typically go beyond parsimony to say that a more\n",
    "complex model must be not just better than, but a specified amount\n",
    "better than, a simpler model. If the more complex model doesnt exceed a\n",
    "threshold of improvement in fit (we will see below exactly where this\n",
    "threshold comes from), we typically reject it in favor of the simpler\n",
    "model.\n",
    "\n",
    "Model complexity also affects our predictive ability. Walters and Ludwig\n",
    "(1981) simulated fish population dynamics using a complex agestructured\n",
    "model and showed that in many cases, when data were realistically sparse\n",
    "and noisy, they could best predict future (simulated) dynamics using a\n",
    "simpler non-age-structured model. In other words, even though they knew\n",
    "for sure that juveniles and adults had different mortality rates\n",
    "(because they simulated the data from a model with mortality\n",
    "differences), a model that ignored this distinction gave more accurate\n",
    "predictions. This apparent paradox is an example of the ***bias-variance\n",
    "tradeoff*** introduced in Chapter 5. As we add more parameters to a\n",
    "model, we necessarily get an increasingly precise fit to the particular\n",
    "data we have observed (the bias decreases), but our accuracy for\n",
    "predicting future observations decreases as well (the variance\n",
    "increases). Data contain a fixed amount of information; as we estimate\n",
    "more and more parameters we spread the data thinner and thinner.\n",
    "Eventually the gain in precision from having more details in the model\n",
    "is outweighed by the loss in accuracy from estimating the effect of each\n",
    "of those details more poorly. In Ludwig and Walterss case, spreading the\n",
    "data out across age classes meant there was not enough data to estimate\n",
    "each age classs dynamics accurately.\n",
    "\n",
    "### Likelihood ratio test: nested models\n",
    "\n",
    "Comparisons among different groups can also be framed as a comparison of\n",
    "nested models. If the more complex model has the mean of group 1 equal\n",
    "to $a1$ and the mean of group 2 equal to $a2$ , then the nested model\n",
    "(both groups equivalent) applies when $a1=a2$ . It is also common to\n",
    "parameterize this model as $a2=a1+\\delta12$ , where $\\delta12=a2-a1$ ,\n",
    "so that the simpler model applies when $\\delta12=0$. This\n",
    "parameterization works better for model comparisons since testing the\n",
    "hypothesis that the more complex model is better becomes a test of the\n",
    "value of one parameter ($\\delta12=0?$) rather than a test of the\n",
    "relationship between two parameters ($a1=a2$ ?).\n",
    "\n",
    "The Likelihood Ratio Test can compare any two nested models, testing\n",
    "whether the nesting parameters of the more complex model differ\n",
    "significantly from their null values. Put another way, the LRT tests\n",
    "whether the extra goodness of fit to the data is worth the added\n",
    "complexity of the additional parameters. To use the LRT to compare\n",
    "models, compare the difference in deviances (the more complex model\n",
    "should always have a smaller deviance if not, check for problems with\n",
    "the optimization) to the critical value of the $\\chi^{2}$ distribution,\n",
    "with degrees of freedom equal to the additional number of parameters in\n",
    "the more complex model. If the difference in deviances is greater than\n",
    "$\\chi_{n2-n1}^{2}(1-\\alpha)$, then the more complex model is\n",
    "significantly better at the $p=\\alpha$ level. If not, then the\n",
    "additional complexity is not justified.\n",
    "\n",
    "Choosing among statistical distributions can often be reduced to\n",
    "comparing among nested models. The most common use of the LRT in this\n",
    "context is to see whether we need to use an *overdispersed distribution\n",
    "such as the negative binomial or beta-binomial* instead of their\n",
    "*lower-variance counterparts (Poisson or binomial)*. **The Poisson\n",
    "distribution is nested in the negative binomial distribution when\n",
    "$k\\rightarrow\\infty$.** If we fit a model with $a$ and $b$ varying but\n",
    "using a Poisson distribution instead of a negative binomial, we can then\n",
    "use the LRT to see if adding the overdispersion parameter is justified:\n",
    "`anova(poisfit.ab, nbfit.ab)`.\n",
    "\n",
    "### Information criteria\n",
    "\n",
    "One way to avoid having to make pairwise model comparisons is to select\n",
    "models based on *information criteria*, which compare all candidate\n",
    "models at once and do not require nested alternatives. These relatively\n",
    "recent alternatives to likelihood ratio tests are based on the expected\n",
    "distance (quantified in a way that comes from information theory)\n",
    "between a particular model and the true model (Burnham and Anderson,\n",
    "1998, 2002). In practice, all information-theoretic methods reduce to\n",
    "the finding the model that minimizes some criterion that is the sum of a\n",
    "term based on the likelihood (usually twice the negative log-likelihood)\n",
    "and a *penalty term* which is different for different information\n",
    "criteria.\n",
    "\n",
    "**For all information criteria, small values represent better overall\n",
    "fits.**\n",
    "\n",
    "The *Akaike Information Criterion*, or AIC, is the most widespread\n",
    "information criterion, and is defined as $AIC=-2L+2k$, where $L$ is the\n",
    "log-likelihood and $k$ is the number of parameters in the model. For\n",
    "small sample sizes (n) such as when n/k \\< 40 (Burnham and Anderson,\n",
    "2004, p. 66)) you should use a finite-size correction and apply the\n",
    "$AIC_{c}=AIC+\\frac{2k(k+1)}{n-k-1}$\n",
    "\n",
    "The second most common information criterion, the *Schwarz criterion* or\n",
    "*Bayesian information criterion* (SC/BIC) , uses a penalty term of\n",
    "$(logn)k$. When n is greater than $e^{2}\\approx9$ observations (so that\n",
    "log n \\> 2), the BIC is more conservative than the AIC, insisting on a\n",
    "greater improvement in fit before it will accept a more complex model.\n",
    "\n",
    "Information criteria do not allow frequentist significance tests based\n",
    "on the estimated probability of getting more extreme results in repeated\n",
    "experiments (some statisticians would say this is an advantage). **With\n",
    "ICs, you cannot say that there is a statistically significant difference\n",
    "between models**; a model with a lower IC is better, but there is no\n",
    "p-value associated with how much better it is. Instead, there are\n",
    "***commonly used rules of thumb***: **models with ICs less than 2 apart\n",
    "($\\Delta$IC \\< 2) are more or less equivalent; those with ICs 4-7 apart\n",
    "are clearly distinguishable; and models with ICs more than 10 apart are\n",
    "definitely different**. Richards (2005) concurs with these\n",
    "recommendations, but cautions that simply dropping models with\n",
    "$\\Delta$AIC \\> 2 (as some ecologists do) will probably discard useful\n",
    "models.\n",
    "\n",
    "One big advantage of IC-based approaches is that they do not require\n",
    "nested models. You can compare all models to each other, rather than\n",
    "stepping through a sometimes confusing sequence of pairwise tests. In\n",
    "ICbased approaches, you simply compute the likelihood and IC for all of\n",
    "the candidate models and rank them in order of increasing IC. The model\n",
    "with the lowest IC is the best fit to the data; those models with ICs\n",
    "within 10 units of the minimum IC are worth considering. As with the\n",
    "LRT, the absolute size of the ICs is unimportant only the differences in\n",
    "ICs matter.\n",
    "\n",
    "### Bayesian analyses\n",
    "\n",
    "Bayesians are on the whole less interested in formal methods of model\n",
    "selection. Dropping a parameter from a model is often equivalent to\n",
    "testing a null hypothesis that the parameter is exactly zero, and\n",
    "Bayesians consider such point null hypotheses silly. They would describe\n",
    "a parameters distribution as being concentrated near zero rather than\n",
    "saying its value is exactly zero.\n",
    "\n",
    "The marginal likelihood of a model is the probability of observing the\n",
    "data (likelihood), averaged over the prior distribution of the\n",
    "parameters: $$\\hat{L}=\\int L(x)\\cdot Prior(x)dx$$ where x represents a\n",
    "parameter or set of parameters (if a set, then the integral would be a\n",
    "multiple integral). The marginal likelihood (the average probability of\n",
    "observing a particular data set exactly ) is often very small, and we\n",
    "are really interested in the relative probability of different models.\n",
    "If we have two models with marginal likelihoods\n",
    "$L1$ and $L2$, the *Bayes factor* is the ratio of the marginal likelihoods,\n",
    "$B12=L1 / L2$, or the odds in favor of model 1.\n",
    "\n",
    "A more recent criterion, conveniently built into WinBUGS, is the DIC or\n",
    "*deviance information criterion*, which was designed particularly for\n",
    "models containing random effects where even specifying the number of\n",
    "parameters is confusing.\n",
    "\n",
    "### Model weighting and averaging\n",
    "\n",
    "Bayesians themselves would say that you should not simply select one\n",
    "model. Taking the best model and ignoring the rest is equivalent to\n",
    "assigning a probability of 1.0 to the best and 0.0 to the rest. Model\n",
    "averaging methods take the average of the predictions of different\n",
    "models, weighted by the probability of the models or by some other\n",
    "index.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
