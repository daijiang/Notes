{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading notes of **Data analysis using regression and multilevel/hierachical models**\n",
    "\n",
    "\n",
    "Chapter 1: Why?\n",
    "====\n",
    "\n",
    "The two key parts of a multilevel model are varying coefficients and a\n",
    "model for those varying coefficients.\n",
    "\n",
    "Fixed effects can be viewed as special cases of random effects, in which\n",
    "the higher level variance is set to 0 or $\\infty$.\n",
    "\n",
    "Chapter 2: Concepts and methods from basic probability and statistics\n",
    "==================================\n",
    "\n",
    "Mean and variance of sums of correlated random variables: If $x$ and $y$\n",
    "are random variables with means $\\mu_{x},\\,\\mu_{y}$, and standard\n",
    "deviations $\\sigma_{x},\\,\\sigma_{y}$, and correlation $\\rho$, then $x+y$\n",
    "has mean $\\mu_{x}+\\mu_{y}$ and standard deviation\n",
    "$\\sqrt{\\sigma_{x}^{2}+\\sigma_{y}^{2}+2\\rho\\sigma_{x}\\sigma_{y}}$. More\n",
    "generally, the weighted sum $ax+by$ has mean $a\\mu_{x}+b\\mu_{y}$ and its\n",
    "standard deviation is\n",
    "$\\sqrt{a^{2}\\sigma_{x}^{2}+b^{2}\\sigma_{y}^{2}+2ab\\rho\\sigma_{x}\\sigma_{y}}$.\n",
    "\n",
    "Estimated regression coefficients are themeselves linear combinations of\n",
    "data: $\\hat{\\beta}=(X^{t}X)^{-1}X^{t}y$, i.e. $\\beta$ is a linear\n",
    "combination of the data values $y$.\n",
    "\n",
    "Two pitfalls of the approach of summarizing by statistical significance:\n",
    "1) statistical significance does not equal practical significance. 2)\n",
    "changes in statistical significance are not themselves significant. Even\n",
    "large changes in significance levels can correspond to small,\n",
    "nonsignificant changes in the underlying variables.\n",
    "\n",
    "Chapter 3: Linear regression: the basics\n",
    "=============================\n",
    "\n",
    "For a binary predictor, the regression coefficient is the difference\n",
    "between the averages of the two groups.\n",
    "\n",
    "With multiple predictors, typical advice is to interpret each\n",
    "coefficient “with all other predictors held constant.” But it is not\n",
    "always possible to change one predictor while holding all others\n",
    "constant. For example, if a model includes both $x$ and $x^{2}$ as\n",
    "predictors, it does not make sense to consider changes in $x$ with\n",
    "$x^{2}$ held constant.\n",
    "\n",
    "Interactions can be important. Including interactions is a way to allow\n",
    "a model to be fit differently to different subsets of data. Models with\n",
    "interactions can often be more easily interpreted if we first\n",
    "pre-process the data by centering each input variable about its mean or\n",
    "some other convenient reference point.\n",
    "\n",
    "The least squares estimate is also the maximum likelihood estimate if\n",
    "the errors $\\epsilon_{i}$ are independent with equal variance and\n",
    "normally distributed. In any case, the least squares estimate can be\n",
    "expressed in matrix notation as $\\hat{\\beta}=(X^{t}X)^{-1}X^{t}y$. As a\n",
    "byproduct of the least squares estimation of $\\beta$, the residuals\n",
    "$r_{i}=y_{i}-X_{i}\\hat{\\beta}$ will be uncorrelated with all the\n",
    "predictors in the model. If the model includes a constant term, then the\n",
    "residuals must be uncorrelated with a constant, which means they must\n",
    "have mean 0. ***This is a byproduct of how the model is estimated, it is\n",
    "not a regression assumption***.\n",
    "\n",
    "Residual standard deviation\n",
    "$\\hat{\\sigma}=\\sqrt{\\sum_{i=1}^{n}r_{i}^{2}/(n-k)}$. The fit of the\n",
    "model can be summarized by $\\hat{\\sigma}$ (the smaller the residual\n",
    "variance, the better the fit) and by $R^{2}$, the fraction of variance\n",
    "“explained” by the model. The unexplained variance is\n",
    "$\\hat{\\sigma}^{2}$, and if we label $s_{y}$ as the sd of the data, then\n",
    "$R^{2}=1-\\hat{\\sigma}^{2}/s_{y}^{2}$. $\\hat{\\sigma}^{2}$ has a sampling\n",
    "distribution centered at the true value, $\\sigma^{2}$, and proportional\n",
    "to a $\\chi^{2}$ distribution with $n-k$ degree of freedom.\n",
    "\n",
    "When an estimate is statistically significant, we are fairly sure that\n",
    "the sign (+/-) of the estimate is stable, and not just an artifact of\n",
    "small sample size.\n",
    "\n",
    "It is fine to have nonsignificant coefficients in a model, as long as\n",
    "they make sense.\n",
    "\n",
    "**Assumptions** of the regression model: Validity (the data you are\n",
    "analyzing should map to the research question you are trying to answer);\n",
    "additivity and linearity (the deterministic component is a linear\n",
    "function of the predictors); independence of errors; equal variance of\n",
    "errors; normality of errors.\n",
    "\n",
    "Chapter 4: Linear regression: before and after fitting the model\n",
    "================================\n",
    "\n",
    "Linear transformations do not affect the fit of a classical regression\n",
    "model, and they do not affect predictions: the changes in the inputs and\n",
    "the coefficients cancel in forming the predicted value $X\\beta$.\n",
    "However, well-chosen linear transformation can improve interpretability\n",
    "of coefficients and make a fitted model easier to understand. For\n",
    "example, in the model $earnings\\sim height+male$, it does not make sense\n",
    "when height is 0. If we centered height at its mean value, then the\n",
    "interpretation is the earning at the mean height.\n",
    "\n",
    "Standardizing predictors using z-scores will change our interpretation\n",
    "of the intercept to the mean of $y$ when all predictor values are at\n",
    "their mean values.\n",
    "\n",
    "We actually prefer to devide by 2 standard deviations to allow\n",
    "inferences to be more consistent with those for binary inputs.\n",
    "\n",
    "Linear transformation of the predictors does not affect the fit of a\n",
    "classical regression model, and the residual sd, $R^{2}$, and the\n",
    "coefficient and standard error of the interaction do not change.\n",
    "\n",
    "Consider a model $y=a+bx+error$, if both $x$ and $y$ are standardized (1\n",
    "sd), then the regression intercept is zero and the slope is simply the\n",
    "correlation between $x$ and $y$. Thus the slope of a regression of two\n",
    "standardized variables must always be between -1 and 1. In general, the\n",
    "slope of a regression with one predictor is\n",
    "$b=\\rho\\sigma_{y}/\\sigma_{x}$.\n",
    "\n",
    "The princial component line: goes theough the cloud of points, in the\n",
    "sense of minimizing the sum of squared Euclidean distances between the\n",
    "points and the line.\n",
    "\n",
    "The regression line: minimizes the sum of the squares of the vertical\n",
    "distances between the points and the line.\n",
    "\n",
    "For the goal of predicting $y$ from $x$, or for estimating the average\n",
    "of $y$ for any given value of $x$, the regression line is in fact\n",
    "better.\n",
    "\n",
    "Regression to the mean: when $x$ and $y$ are standardized (placed on a\n",
    "common scale), the regression line always has slope less than 1. Thus\n",
    "when $x$ is 1 sd above the mean, the predicted value of $y$ is somewhere\n",
    "between 0 and 1 sd above the mean. This phonomenon in linear models,\n",
    "that $y$ is predicted to be closer to the mean (in sd units) than $x$,\n",
    "is called regression to the mean. A naive interpretation of regression\n",
    "to the mean is that heights or other variable phenomena necessarily\n",
    "become more and more average over time. This view is mistaken because it\n",
    "ignores the error in the regression predicting $y$ from $x$. For any\n",
    "data point $x_{i}$, the point prediction for its $y_{i}$ will be\n",
    "regressed toward the mean, but the actual $y_{i}$ that is observed will\n",
    "not be exactly where it is predicted.\n",
    "\n",
    "A linear model on the logarithmic scale corresponds to a multiplicative\n",
    "model on the original scale.\n",
    "\n",
    "Log-log model: the coefficient can be interpreted as the expected\n",
    "proportional changes in $y$ per proportional change in $x$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
