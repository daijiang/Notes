{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Chapter 2 General linear model\n",
    "\n",
    "### Classic inference for the traditional linear statistical model\n",
    "\n",
    "$$y = X\\beta + \\mathbf{e}$$\n",
    "where y is a $T \\times 1$ vector of observed values of the random variable y on a sample space defined on the Euclidean line. X is a know $T \\times K$ nonstochastic design matrix of rank $K$, $\\beta$ is a K-dimensional fixed vector of unknown paramters and $\\mathbf{e}$ is a $T \\times 1$ vector of unobservable random variables with mean vector $E[\\mathbf{e}]=0$ and finite covariane matrix. Assume $\\mathbf{e}$ to be iid, then the covariance matrix is $E[ee']=\\sigma^2I_T$.\n",
    "\n",
    "The assumption that X is a matrix of fixed variables means that for all the possible situations that might take place **under repeated sampling**, the matrix X would take on the same values.\n",
    "\n",
    "#### The least squars rules\n",
    "The observed values of the random variables y contain **all** pf the information available about the unknown $\\beta$ vector and the unknown scalar $\\sigma^2$. Consequently, the point estimation problem is one of finding a suitable function of the observed values of the random variables y, given the design matrix X, that will yield, **in a repeated sample sense**, the \"best\" estimator of the unknown parameters. If, when estimate $\\beta$, we restrict to the class of rules that are linear functions of y, we may write the general linear estimator as \n",
    "$$b_0 = Ay$$\n",
    "where A is some $K \\times T$ matrix. This means that the estimation problem is reduced to determining a matrix A that \"appropriately\" **summarizes the information contained in y**. Because $b_0$ is a linear function of the random vector y, it is also a random vector.\n",
    "\n",
    "*Sampling theory approach*:\n",
    "1. Select a criterion;\n",
    "2. determine the matrix A\n",
    "3. evaluate the sampling performance of the estimator: **unbiasedness, efficiency, invariance**.\n",
    "\n",
    "The least squares **point estimator of $\\beta$** is \n",
    "$$b = (X'X)^{-1}X'y$$\n",
    "\n",
    "$$E[b]=E[(X'X)^{-1}X'y]=\\beta + E[(X'X)^{-1}X'e]=\\beta : Unbiased$$\n",
    "\n",
    "Precision (variance-covariance) matrix:\n",
    "$$\\Sigma_b = E[(b-E(b))(b-E(b))']=\\sigma^2(X'X)^{-1}$$\n",
    "\n",
    "The Gauss-Markov theorem provides proof that out of the *class of linear unbiased rules*, the least squares estimator is *the best* in terms of minimum variance. In another word, the least squares estimator is equal to or better in terms of sampling precision than all others in its class.\n",
    "\n",
    "**Estimator of $\\sigma^2$**\n",
    "From above, we know that $E[ee']=\\sigma^2I_T$ and $E[e'e]=T\\sigma^2$. Thus, if we have a sample of obervations for the random error vector $y-X\\beta=e$, we would use this sample as a basis for estimation of $\\sigma^2$. Unfortunately, since $\\beta$ is **unknown and unobservable the random vector $e$ is unobservable**. Thus, the estimation will involve the observed random y vector and its least squares $\\hat{y}=Xb$. The vector of least squares residuals $\\hat{e}=y=Xb$ provides a **least squares analogue** of the vector of unobservable error $e$.\n",
    "\n",
    "$$E[\\hat{e}'\\hat{e}]=E[e'(I_T - X(X'X)^{-1}X')e]=\\sigma^2(T-K)$$\n",
    "\n",
    "$$\\hat{\\sigma}^2 = E[\\frac{\\hat{e}'\\hat{e}}{T-K}] = \\sigma^2 : unbiased$$\n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\frac{(y-Xb)'(y-Xb)}{T-K} = \\frac{y'(I_T-X(X'X)^{-1}X')y}{T-K}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Maximum likelihood rule\n",
    "\n",
    "Under the maximum likelihood criterion, the parameter estimates are chosen so as to maximize the probability of generating or obtaining the observed sample.\n",
    "\n",
    "$$L(\\beta,\\sigma | \\mathbf{y}) = \\frac{1}{(2\\pi\\sigma^{2})^{n/2}} \\exp\\left(\\frac{-(y-X\\beta)'(y-X\\beta)}{2\\sigma^2}\\right)$$\n",
    "\n",
    "$$\\tilde{\\beta} = (X'X)^{-1}X'y=b : same\\ as\\ least\\ squres$$\n",
    "\n",
    "$$\\tilde{\\sigma}^2=\\frac{(y-X\\tilde{\\beta})'(y-X\\tilde{\\beta})}{T}$$\n",
    "\n",
    "The point estimate $\\tilde{\\beta}$ is a best linear unbiased estimator, same as that from least squares rule.\n",
    "\n",
    "$$E[\\tilde{\\sigma}^2] = \\sigma^2\\frac{(T-K)}{T}$$ and thus is biased in finite samples. However, the estimator $[T/(T-K)]\\tilde{\\sigma}^2 = \\hat{\\sigma}^2$ is an unbiased estimator. \n",
    "\n",
    "The term $(T-K)\\hat{\\sigma}^2/\\sigma^2$ is distributed as a chi-square random variable with $(T_K)$ degrees of freedom, i.e $(T-K)\\hat{\\sigma}^2/\\sigma^2 \\sim \\chi^2_{(T_K)}$.\n",
    "\n",
    "$$E[\\hat{\\sigma}^{2}]=\\frac{\\sigma^{2}}{T-K}E[\\chi_{(T-K)}^{2}]=\\sigma^{2}$$\n",
    "\n",
    "$$E[(\\hat{\\sigma}^{2}-E([\\hat{\\sigma}^{2}])^{2}]=E[(\\hat{\\sigma}^{2}-\\sigma^{2})^{2}]=\\frac{\\sigma^{4}}{(T-K)^{2}}Var(\\chi_{(T-K)}^{2})=2\\sigma^{4}/(T-K)$$\n",
    "\n",
    "$$E[\\tilde{\\sigma}^{2}]=\\frac{\\sigma^{2}}{T}E[\\chi_{(T-K)}^{2}]=\\frac{T-K}{T}\\sigma^{2}$$\n",
    "\n",
    "$$E[(\\tilde{\\sigma}^{2}-E([\\tilde{\\sigma}^{2}])^{2}]=\\frac{\\sigma^{4}}{T^{2}}Var(\\chi_{(T-K)}^{2})=2\\sigma^{4}(T-K)/T^2$$\n",
    "\n",
    "Thus $\\tilde{\\sigma}^{2}$ has a bias $-(K/T)\\sigma^2$ but its variance in finite samples is smaller than the variance for the unbiased estimator $\\hat{\\sigma}^{2}$, i.e.  there is a **trade-off between bias and precision**. \n",
    "\n",
    "To get a minimum mean squared error (MSE) estimator of $\\sigma^2$, that is a esimator $\\bar{\\sigma}^2$ where\n",
    "$$E[(\\bar{\\sigma}^2 - \\sigma^2)^2]=variance(\\bar{\\sigma}^2) + bias(\\bar{\\sigma}^2)^2$$ is minimum. It turns out \n",
    "$$\\bar{\\sigma}^2 = \\hat{\\sigma}^2\\frac{T-K}{T-K+2}$$\n",
    "Its bias is $\\sigma^{2}(T-K)/(T-K+2)$ and variance is $2\\sigma^{4}(T-K)/(T-K+2)^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
